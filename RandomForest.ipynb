{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0014e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation des bibliothèques\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import scipy.special\n",
    "\n",
    "\n",
    "###### Ici, on importe et traite les données #########\n",
    "\n",
    "#Importation des données\n",
    "data_training = pd.read_csv('sign_mnist_train.csv').to_numpy()\n",
    "data_test = pd.read_csv('test.csv').to_numpy() \n",
    "\n",
    "# Division entre cibles et données (entrainement)\n",
    "cibles = data_training[:, 0]\n",
    "images_train = data_training[:, 1:]\n",
    "\n",
    "# Division des deux images dans les données de test\n",
    "image1_test = data_test[:, :784]\n",
    "image2_test = data_test[:, 785:]\n",
    "\n",
    "#Correction de la translation + bruit sur la première image de test\n",
    "image1_test = image1_test.reshape(3000, 28, 28)\n",
    "image1_test[:, :, :-1] = image1_test[:, :,1:]\n",
    "image1_test = image1_test.reshape(3000, (784))\n",
    "\n",
    "# Greyscale values -> [0,1]\n",
    "\n",
    "images_train = (images_train - np.mean(images_train, axis = 1)[:, np.newaxis])/255\n",
    "image1_test = (image1_test - np.mean(image1_test, axis = 1)[:, np.newaxis])/255\n",
    "image2_test = (image2_test - np.mean(image2_test, axis = 1)[:, np.newaxis])/255\n",
    "\n",
    "# Reconstitution des tableaux pour les images\n",
    "images_train = images_train.reshape((len(images_train),28,28))\n",
    "image1_test = image1_test.reshape((len(image1_test),28,28))\n",
    "image2_test = image2_test.reshape((len(image2_test),28,28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4914ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cette partie permet définie la fonction qui prend les images et donne leur décomposition en 'mode de Krawtchouk'.\n",
    "Ces modes sont analogues aux séries de Fourier.\n",
    "'''\n",
    "\n",
    "##### Fonctions utiles #######\n",
    "\n",
    "def poch(n,k):\n",
    "    prod = 1\n",
    "    for i in range(k):\n",
    "        prod = prod *(n+i)\n",
    "    return prod\n",
    "\n",
    "def hypergeo2f1(a,b,c,x):\n",
    "    somme = 0\n",
    "    if ((a !=c) or (b !=c)) :\n",
    "        for i in range(int(min([-a,-b]))+1):\n",
    "            somme += poch(a,i)*poch(b,i)*(x)**i /(poch(c,i)*math.factorial(i))\n",
    "    else:\n",
    "        for i in range(int(min([-a,-b]))+1):\n",
    "            somme += poch(a,i)*(x)**i /(math.factorial(i))\n",
    "    return somme\n",
    "\n",
    "def Kraw(n,k,N):\n",
    "    return ((1/2)**(N/2)) *np.sqrt( scipy.special.binom(N, n) * scipy.special.binom(N, k)) *hypergeo2f1(-n,-k,-N,2) \n",
    "\n",
    "\n",
    "## Tableau contenant les vecteurs pour chaque mode de Krawtchouk\n",
    "K = np.array([[Kraw(n,k,27) for n in range(27 + 1)] for k in range(27 + 1)])\n",
    "\n",
    "#Fonction qui prend une image et redonne sa décomposition en mode de Krawtchouk\n",
    "def ImageTransform(Image, kx, ky):\n",
    "    Kx = K[kx,:].reshape(1,28)\n",
    "    Ky = K[ky,:].reshape(28, 1)\n",
    "    filtre = Ky @ Kx\n",
    "    return np.sum(filtre*Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf1a3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin - (images entraînement)\n",
      "Fin - (images test 1)\n",
      "Fin - (images test 2)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Ici, on prend les données pour chaque image et on calcule sa décomposition en mode de Krawtchouk.\n",
    "L'amplitude de ces modes de Krawtchouk formeront les attributs utilisé par les modèles.\n",
    "'''\n",
    "images_train_kraw = np.zeros_like(images_train) # Tableau qui contiendra les amplitudes de Krawtchouk (train)\n",
    "image1_test_kraw = np.zeros_like(image1_test) # Tableau qui contiendra les amplitudes de Krawtchouk (1e image, test)\n",
    "image2_test_kraw = np.zeros_like(image2_test) # Tableau qui contiendra les amplitudes de Krawtchouk (2e image, test)\n",
    "\n",
    "\n",
    "for choice_im in range(images_train.shape[0]):\n",
    "    table = np.zeros((28,28))\n",
    "    for kx in range(28):\n",
    "        for ky in range(28):\n",
    "            table[ky,kx] = ImageTransform( images_train[choice_im], kx, ky)\n",
    "    images_train_kraw[choice_im, :, :] = table\n",
    "print('Fin - (images entraînement)')\n",
    "\n",
    "for choice_im in range(image1_test.shape[0]):\n",
    "    table = np.zeros((28,28))\n",
    "    for kx in range(28):\n",
    "        for ky in range(28):\n",
    "            table[ky,kx] = ImageTransform( image1_test[choice_im], kx, ky)\n",
    "    image1_test_kraw[choice_im, :, :] = table\n",
    "print('Fin - (images test 1)')\n",
    "\n",
    "for choice_im in range(image2_test.shape[0]):\n",
    "    table = np.zeros((28,28))\n",
    "    for kx in range(28):\n",
    "        for ky in range(28):\n",
    "            table[ky,kx] = ImageTransform(image2_test[choice_im], kx, ky)\n",
    "    image2_test_kraw[choice_im, :, :] = table\n",
    "print('Fin - (images test 2)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc8aa6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "À partir d'ici, on défini les classes implémentant le modèle de forêt aléatoire.\n",
    "\n",
    "1- D'abord, on utilise et modifie les classes associés à un classifieur par arbre de décision\n",
    "   utilisée dans le TP7, voir https://colab.research.google.com/drive/1djfzz7iRIVeIT7WtDZ2OXgOPaEtL1k55.\n",
    "\n",
    "2- Ensuite, on définit une classe 'randomForest' qui combine plusieurs de ces arbres pour faire\n",
    "   un modèle de forêt aléatoire.\n",
    "\n",
    "'''\n",
    "\n",
    "# Classifieur pour un arbre unique \n",
    "\n",
    "#Fonction d'aide\n",
    "def histo_from_labels(labels):\n",
    "    labels_list = np.unique(labels).tolist()\n",
    "    labels_list.append(27)\n",
    "    return np.histogram(labels, bins = labels_list)\n",
    "\n",
    "def entropy_from_distribution(dist):\n",
    "    dist = dist/np.sum(dist)\n",
    "    return np.sum(- dist * np.log(dist))\n",
    "\n",
    "def entropy_from_test(distT, distF):\n",
    "    NT = np.sum(distT)\n",
    "    NF = np.sum(distF)\n",
    "    N = NT + NF\n",
    "    return (NT/N)* entropy_from_distribution(distT) + (NF/N)* entropy_from_distribution(distF) \n",
    "    \n",
    "###########  Classe pour chaque noeud  #####################\n",
    "\n",
    "class Node():\n",
    "    def __init__(self):\n",
    "        self.threshold = None\n",
    "        self.col = None\n",
    "        self.is_leaf = None\n",
    "        self.output_class = None\n",
    "        self.left_child=None\n",
    "        self.right_child=None\n",
    "        self.real_depth = None\n",
    "\n",
    "    def find_best_question(self, x, y):\n",
    "        best_col = 0\n",
    "        best_val = 0\n",
    "        best_loss = np.inf\n",
    "\n",
    "        num_cols = x.shape[1]\n",
    "        valid_cols = np.arange(num_cols) # nb of features\n",
    "        for col in valid_cols:\n",
    "            # Calculer les valeurs intermédiaires de cette colonne ici\n",
    "            sorted_indices = x[:, col].argsort()\n",
    "            sorted_vals = x[sorted_indices, col]\n",
    "            midpoints = [(sorted_vals[i]+sorted_vals[i+1])/2 for i in range(len(sorted_vals)-1)]\n",
    "\n",
    "            for val in midpoints:\n",
    "\n",
    "                test = (x[:,col] > val)\n",
    "                \n",
    "                coord_true = np.nonzero(test.astype(int))\n",
    "                coord_false = np.nonzero(np.invert(test).astype(int))\n",
    "\n",
    "                distT = histo_from_labels(y[coord_true])[0]\n",
    "                distF = histo_from_labels(y[coord_false])[0]\n",
    "\n",
    "                loss =  entropy_from_test(distT, distF)\n",
    "\n",
    "\n",
    "                if len(coord_true) == 0 or len(coord_false) == 0:\n",
    "                    continue\n",
    "\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    best_col = col\n",
    "                    best_val = val\n",
    "\n",
    "        self.col = best_col\n",
    "        self.threshold = best_val\n",
    "\n",
    "    def ask_question(self, x):\n",
    "        if not self.is_leaf:\n",
    "            return np.nonzero((x[:, self.col] > self.threshold).astype(int))[0],np.nonzero((x[:, self.col] <= self.threshold).astype(int))[0]\n",
    "        else:\n",
    "            print(\"Error: leaf nodes cannot ask questions!\")\n",
    "            return False\n",
    "\n",
    "    def predict(self):\n",
    "        return self.output_class\n",
    "\n",
    "############## Classe pour l'arbre ##########################\n",
    "\n",
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.actual_depth = 0\n",
    "\n",
    "    def create_node(self, x_subset, y_subset, depth):\n",
    "        # Recursive function\n",
    "        self.actual_depth = max(self.actual_depth, depth)\n",
    "        node = Node()\n",
    "        node.real_depth = depth\n",
    "        \n",
    "        histy = histo_from_labels(y_subset)\n",
    "        \n",
    "        \n",
    "        majority_class = histy[1][np.argmax(histy[0])]\n",
    "        #print('mc:', majority_class)\n",
    "        majority_class_count = (y_subset == majority_class).sum()\n",
    "        perfectly_classified = majority_class_count == len(y_subset)\n",
    "        \n",
    "        #modif\n",
    "        node.output_class = majority_class\n",
    "        \n",
    "        if perfectly_classified or depth == self.max_depth:\n",
    "            node.output_class = majority_class\n",
    "            node.is_leaf = True\n",
    "        else:\n",
    "            node.find_best_question(x_subset,y_subset)\n",
    "            node.is_leaf = False\n",
    "            coord_true_sub,coord_false_sub  = node.ask_question(x_subset)\n",
    "  \n",
    "            node.left_child = self.create_node(x_subset[coord_true_sub],y_subset[coord_true_sub],depth+1) \n",
    "            node.right_child = self.create_node(x_subset[coord_false_sub],y_subset[coord_false_sub], depth+1) \n",
    "\n",
    "        return node\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.root_node = self.create_node(x,y,depth=1)\n",
    "        #print('profondeur finale:', self.actual_depth)\n",
    "\n",
    "    def predict(self, x, cut_descend = None):\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            current_node = self.root_node\n",
    "            x_i = x[i, :]\n",
    "            done_descending_tree = False\n",
    "            while not done_descending_tree:\n",
    "                if current_node.is_leaf or cut_descend == current_node.real_depth:\n",
    "                    predictions.append(current_node.predict())\n",
    "                    done_descending_tree = True\n",
    "\n",
    "                else:\n",
    "                    if len(current_node.ask_question(np.array([x_i]))[0]) == 0 :\n",
    "                        current_node = current_node.right_child\n",
    "                    else:\n",
    "                        current_node = current_node.left_child\n",
    "\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edbd9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifieur pour une forêt aléatoire\n",
    "\n",
    "class randomForest():\n",
    "    def __init__(self, nbarbres, nbfeatures, nbexamples, profondeurs):\n",
    "        self.nbarbres = nbarbres\n",
    "        self.nbfeatures = nbfeatures\n",
    "        self.nbexamples = nbexamples\n",
    "        self.profondeurs = profondeurs\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        nbdata, nbattribut = x.shape\n",
    "        trees = []\n",
    "        subargs = []\n",
    "        for tree in range(self.nbarbres):\n",
    "            #print('fitting tree number:', tree)\n",
    "            classifieur_arbre = DecisionTreeClassifier(max_depth = self.profondeurs)\n",
    "            \n",
    "            sub_ex_args = np.random.randint(nbdata, size=self.nbexamples)\n",
    "            sub_att_args = np.random.randint(nbfeatures, size=self.nbfeatures)\n",
    "            \n",
    "            subargs.append(sub_att_args)\n",
    "\n",
    "            subx = (x[sub_ex_args, :])[:, sub_att_args]\n",
    "            suby = y[sub_ex_args]\n",
    "            classifieur_arbre.fit(subx,suby)\n",
    "            \n",
    "            trees.append(classifieur_arbre)\n",
    "        print('done training')\n",
    "        self.trees = trees\n",
    "        self.subargs = subargs\n",
    "        \n",
    "    def VecteurPredictions(self, x, fractionTrees = 1, cut_desc = None):\n",
    "        vecteur = []\n",
    "        for index_tree in range(int(fractionTrees*len(self.trees))):\n",
    "            tree = self.trees[index_tree]\n",
    "            args = self.subargs[index_tree]\n",
    "            vecteur.append(tree.predict(x[:, args], cut_descend = cut_desc))\n",
    "        return np.array(vecteur)\n",
    "    \n",
    "    def PredictionsFinals(self, x, fT = 1, cd = None):\n",
    "        num_lettres = [i for i in range(27)]\n",
    "        vec = self.VecteurPredictions(x,fractionTrees = fT, cut_desc = cd)\n",
    "        preds = []\n",
    "        for image in range(len(x)):\n",
    "            histo = np.histogram(vec[:,image], bins = num_lettres)\n",
    "            preds.append(num_lettres[np.argmax(histo[0])])\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e6a4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "À partir d'ici, on entraîne et test le modèle.\n",
    "'''\n",
    "###### Division des données en entraînement et validation ##################\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images_train_kraw, cibles, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94922a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbarbres = 10\n",
      "nbfeatures = 15\n",
      "nbexamples = 100\n",
      "profondeurs = 5\n",
      "done training\n",
      "Success rate training = 0.14637588781642688\n",
      "Success rate validation = 0.14059369877982153\n",
      " ---------- \n",
      "nbarbres = 10\n",
      "nbfeatures = 15\n",
      "nbexamples = 100\n",
      "profondeurs = 7\n",
      "done training\n",
      "Success rate training = 0.15070114733199783\n",
      "Success rate validation = 0.14696776543434711\n",
      " ---------- \n",
      "nbarbres = 10\n",
      "nbfeatures = 15\n",
      "nbexamples = 100\n",
      "profondeurs = 9\n",
      "done training\n",
      "Success rate training = 0.15202148971043525\n",
      "Success rate validation = 0.14059369877982153\n",
      " ---------- \n",
      "nbarbres = 10\n",
      "nbfeatures = 15\n",
      "nbexamples = 100\n",
      "profondeurs = 11\n",
      "done training\n",
      "Success rate training = 0.17738116918594063\n",
      "Success rate validation = 0.17792751775632853\n",
      " ---------- \n"
     ]
    }
   ],
   "source": [
    "### Entraînement ###\n",
    "\n",
    "#Grille de recherche pour l'optimisation des hyperparamètres\n",
    "li_nbarbres = [10] # [10, 25, 50, 100, 150, 200]  \n",
    "li_nbfeatures = [15] #[15]\n",
    "li_nbexamples = [100] #[100, 200, 500, 1000]\n",
    "li_profondeurs = [5,7,9,11] #[5,7,9,11]\n",
    "\n",
    "success_rate_train = []\n",
    "success_rate_valid = []\n",
    "\n",
    "# Gridsearch\n",
    "for w in li_nbarbres:\n",
    "    for x in li_nbfeatures:\n",
    "        for y in li_nbexamples:\n",
    "            for z in li_profondeurs:\n",
    "                nbarbres, nbfeatures, nbexamples, profondeurs = w,x,y,z\n",
    "                print('nbarbres = ' + str(nbarbres))\n",
    "                print('nbfeatures = ' + str(nbfeatures))\n",
    "                print('nbexamples = ' + str(nbexamples))\n",
    "                print('profondeurs = ' + str(profondeurs))\n",
    "                \n",
    "                model = randomForest(nbarbres, nbfeatures, nbexamples, profondeurs)\n",
    "                model.fit(X_train.reshape((len(X_train), (28 )**2)), y_train)\n",
    "                \n",
    "                #Calcul du taux de succès d'entrainement\n",
    "                predictions_train = model.PredictionsFinals(X_train.reshape((len(X_train), (28 )**2)), fT = 0.5 , cd = 5)\n",
    "                success_rate_train.append(np.sum((predictions_train == y_train).astype(int))/len(predictions_train))\n",
    "                \n",
    "                #Calcul du taux de succès de test\n",
    "                predictions_test = model.PredictionsFinals(X_test.reshape((len(X_test),(28)**2)),fT = 0.5 , cd = 5)\n",
    "                success_rate_valid.append(np.sum((predictions_test == y_test).astype(int))/len(predictions_test))\n",
    "                \n",
    "                print('Success rate training = ' + str(success_rate_train[-1]))\n",
    "                print('Success rate validation = ' + str(success_rate_valid[-1]))\n",
    "                print(' ---------- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a0a2d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ici, on applique le meilleur modèle sur les données de test et on produit les prédictions pour Kaggle\n",
    "'''\n",
    "\n",
    "# Entraînement du modèle avec les meilleures performances en validation\n",
    "best_nbarbres, best_nbfeatures, best_nbexamples, best_profondeurs = 100, 15,1000,11 #meilleurs paramètres\n",
    "model = randomForest(best_nbarbres, best_nbfeatures, best_nbexamples, best_profondeurs)\n",
    "model.fit(X_train.reshape((len(X_train), (28 )**2)), y_train)\n",
    "\n",
    "\n",
    "### Fonction qui somme les ASCII de deux images\n",
    "# Nombre entre 0 et 26 vers charactère ASCII\n",
    "def SumASCII(arr1, arr2):\n",
    "    \n",
    "    #Somme des ASCII des deux charactères et retrait des 65 pour int [65,122]\n",
    "    arrSUM = np.array(arr1) + np.array(arr2) + 65\n",
    "    \n",
    "    #Somme vers charactères\n",
    "    char_list = []\n",
    "    for num in arrSUM:\n",
    "        char_list.append(chr(num))\n",
    "        \n",
    "    return np.array(char_list)\n",
    "\n",
    "#Predictions pour la première image de chaque paire\n",
    "predictions_test1 =  model.PredictionsFinals(image1_test_kraw.reshape((len(image1_test_kraw), (28 )**2)), cd = 10)\n",
    "\n",
    "#Predictions pour la seconde image de chaque paire\n",
    "predictions_test2 = model.PredictionsFinals(image2_test_kraw.reshape((len(image2_test_kraw), (28 )**2)), cd = 10)\n",
    "\n",
    "#Prediction du charactère latin correspondant à la somme ASCII\n",
    "predictions_char = SumASCII(predictions_test1,predictions_test2)\n",
    "\n",
    "###############################################################################\n",
    "# Création du fichier csv de prédictions pour Kaggle\n",
    "\n",
    "test_pred = np.c_[ np.reshape(np.array(range(len(predictions_char ))), (len(predictions_char ), 1)),np.reshape(np.array(predictions_char ), (len(predictions_char), 1))]\n",
    "df = pd.DataFrame(test_pred, columns = ['id', 'Label'])\n",
    "df.to_csv(\"pred_rf.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7350eec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
