{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_file_path = 'sign_mnist_train2.csv'\n",
        "test_file_path = 'test2.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_file_path = 'https://raw.githubusercontent.com/Herbrax/Kaggle_MNIST_Sign/main/sign_mnist_train2.csv'\n",
        "test_file_path = 'https://raw.githubusercontent.com/Herbrax/Kaggle_MNIST_Sign/main/test2.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_file_path = 'sign_mnist_train.csv'\n",
        "test_file_path = 'test.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f6AJypx9VdU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "\n",
        "# Setting a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "split_ratio = 0.8\n",
        "\n",
        "###################### -- HELPER FUNCTIONS -- ######################\n",
        "\n",
        "def separate_test_sets(file_path):\n",
        "    test_data = pd.read_csv(file_path)\n",
        "    test_a_columns = [col for col in test_data.columns if 'pixel_a' in col]\n",
        "    test_b_columns = [col for col in test_data.columns if 'pixel_b' in col]\n",
        "    test_a = test_data[test_a_columns]\n",
        "    test_b = test_data[test_b_columns]\n",
        "    test_a.columns = [col.replace('_a', '') for col in test_a.columns]\n",
        "    test_b.columns = [col.replace('_b', '') for col in test_b.columns]\n",
        "    return test_a, test_b\n",
        "\n",
        "def predict_and_merge(model, test_a, test_b):\n",
        "    preds_a = model.compute_predictions(test_a)\n",
        "    preds_b = model.compute_predictions(test_b)\n",
        "    merged_predictions = []\n",
        "    for i in range(len(preds_a)):\n",
        "        # Replacing 9 back to 24 if needed\n",
        "        decoded_a = preds_a[i] if preds_a[i] != 9 else 24\n",
        "        decoded_b = preds_b[i] if preds_b[i] != 9 else 24\n",
        "\n",
        "        ascii_a = decoded_a + 65\n",
        "        ascii_b = decoded_b + 65\n",
        "        sum_pred = normalize_ascii_sum(ascii_a + ascii_b)\n",
        "        merged_predictions.append((i, chr(sum_pred)))\n",
        "    return merged_predictions\n",
        "\n",
        "\n",
        "def predict_and_merge2(model, test_a, test_b):\n",
        "    preds_a = model.compute_predictions(test_a)\n",
        "    preds_b = model.compute_predictions(test_b)\n",
        "    merged_predictions = []\n",
        "    for i in range(len(preds_a)):\n",
        "        decoded_a = preds_a[i] if preds_a[i] != 9 else 24\n",
        "        decoded_b = preds_b[i] if preds_b[i] != 9 else 24\n",
        "        sum_pred = decoded_a + decoded_b\n",
        "        print(decoded_a,\";\",decoded_b,\"----------------------\",sum_pred)\n",
        "        merged_predictions.append((i, sum_pred))\n",
        "    return merged_predictions\n",
        "\n",
        "def save_predictions_to_csv(filename, predictions):\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(\"id,label\\n\")\n",
        "        for id, label in predictions:\n",
        "            file.write(f\"{id},{label}\\n\")\n",
        "\n",
        "def normalize_ascii_sum(ascii_sum):\n",
        "    while ascii_sum > 122:  # 'z' is ASCII 122\n",
        "        ascii_sum -= 65  # 122 ('z') - 65 ('A') + 1\n",
        "    return int(ascii_sum)\n",
        "\n",
        "def plot_results_XGB(results):\n",
        "    learning_rates = [x[0] for x in results]\n",
        "    max_depths = [x[1] for x in results]\n",
        "    accuracies = [x[2] for x in results]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for lr in set(learning_rates):\n",
        "        specific_lr_depths = [depth for depth, l_rate in zip(max_depths, learning_rates) if l_rate == lr]\n",
        "        specific_acc = [acc for acc, l_rate in zip(accuracies, learning_rates) if l_rate == lr]\n",
        "        plt.plot(specific_lr_depths, specific_acc, label=f'Learning Rate {lr}')\n",
        "    plt.title('Accuracy for different max depths and learning rates')\n",
        "    plt.xlabel('Max Depth')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "###################### -- Data Handling -- ######################\n",
        "\n",
        "mnist_sign_train = pd.read_csv(train_file_path)\n",
        "mnist_sign_train['label'] = mnist_sign_train['label'].replace(24, 9)\n",
        "mnist_sign_train = mnist_sign_train.sample(frac=1).reset_index(drop=True)\n",
        "labels = mnist_sign_train['label'].values\n",
        "features = mnist_sign_train.drop('label', axis=1).values\n",
        "\n",
        "features_normalized = features / 255.0\n",
        "split_index = int(split_ratio * len(features_normalized))\n",
        "train_data, validation_data = features_normalized[:split_index], features_normalized[split_index:]\n",
        "train_labels, validation_labels = labels[:split_index], labels[split_index:]\n",
        "\n",
        "train_mean = train_data.mean(axis=0)\n",
        "train_std = train_data.std(axis=0)\n",
        "train_data_normalized = (train_data - train_mean) / train_std\n",
        "validation_data_normalized = (validation_data - train_mean) / train_std\n",
        "\n",
        "test_a, test_b = separate_test_sets(test_file_path)\n",
        "normalized_test_a = (test_a.values / 255.0 - train_mean) / train_std\n",
        "normalized_test_b = (test_b.values / 255.0 - train_mean) / train_std\n",
        "\n",
        "###################### -- XGBOOST Implementation -- ######################\n",
        "\n",
        "class XGBoostClassifier:\n",
        "    def __init__(self, max_depth, eta, num_class):\n",
        "        self.params = {\n",
        "            'objective': 'multi:softmax',\n",
        "            'num_class': num_class,\n",
        "            'booster': 'gbtree',\n",
        "            'eval_metric': 'merror',\n",
        "            'eta': eta,\n",
        "            'max_depth': max_depth,\n",
        "        }\n",
        "\n",
        "    def train(self, train_data, train_labels, validation_data, validation_labels):\n",
        "        dtrain = xgb.DMatrix(train_data, label=train_labels)\n",
        "        dval = xgb.DMatrix(validation_data, label=validation_labels)\n",
        "        watchlist = [(dtrain, 'train'), (dval, 'validation')]\n",
        "        self.model = xgb.train(self.params, dtrain, num_boost_round=200, evals=watchlist, early_stopping_rounds=20, verbose_eval=False)\n",
        "\n",
        "    def compute_predictions(self, data):\n",
        "        ddata = xgb.DMatrix(data)\n",
        "        return self.model.predict(ddata)\n",
        "\n",
        "    def compute_accuracy(self, preds, labels):\n",
        "        return np.mean(preds == labels)\n",
        "\n",
        "###################### -- TRAINING -- ######################\n",
        "\n",
        "def startXGBOOST(train_data_normalized, train_labels, validation_data_normalized, validation_labels):\n",
        "    learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "    max_depths = [3, 4, 5, 6, 7, 8, 9, 10]\n",
        "    learning_rates = [0.3]\n",
        "    max_depths = [4]\n",
        "    results_XGB = []\n",
        "    best_acc_xgb = 0\n",
        "    best_lr = None\n",
        "    best_depth = None\n",
        "    num_class = len(np.unique(train_labels))\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        for depth in max_depths:\n",
        "            xgb_model = XGBoostClassifier(max_depth=depth, eta=lr, num_class=num_class)\n",
        "            xgb_model.train(train_data_normalized, train_labels, validation_data_normalized, validation_labels)\n",
        "            val_preds = xgb_model.compute_predictions(validation_data_normalized)\n",
        "            acc = xgb_model.compute_accuracy(val_preds, validation_labels)\n",
        "            print(f\"LR: {lr} ; Depth: {depth} ; Accuracy: {acc * 100:.2f}%\")\n",
        "            if acc > best_acc_xgb:\n",
        "                best_acc_xgb = acc\n",
        "                best_lr = lr\n",
        "                best_depth = depth\n",
        "                print(\"New best\")\n",
        "            results_XGB.append((lr, depth, acc))\n",
        "\n",
        "    #plot_results_XGB(results_XGB)\n",
        "\n",
        "    best_xgb_model = XGBoostClassifier(max_depth=best_depth, eta=best_lr, num_class=num_class)\n",
        "    best_xgb_model.train(train_data_normalized, train_labels, validation_data_normalized, validation_labels)\n",
        "\n",
        "    return best_xgb_model,best_lr,best_depth\n",
        "\n",
        "#best_xgb_model = startXGBOOST(train_data_normalized, train_labels, validation_data_normalized, validation_labels)\n",
        "\n",
        "#final_predictions = predict_and_merge(best_xgb_model, normalized_test_a, normalized_test_b)\n",
        "\n",
        "#save_merged_predictions_to_csv(\"merged_predictions.csv\", final_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sign_mnist_train2.csv\n",
            "test2.csv\n",
            "LR: 0.3 ; Depth: 4 ; Accuracy: 83.50%\n",
            "New best\n"
          ]
        }
      ],
      "source": [
        "print(train_file_path)\n",
        "print(test_file_path)\n",
        "best_xgb_model, best_lr, best_depth = startXGBOOST(train_data_normalized, train_labels, validation_data_normalized, validation_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "akwod-Qh9VdU",
        "outputId": "19d5c8c0-353a-4566-c43f-8abc6aa87055"
      },
      "outputs": [],
      "source": [
        "output = f\"{best_lr}_{best_depth}.csv\"\n",
        "final_predictions = predict_and_merge(best_xgb_model, normalized_test_a, normalized_test_b)\n",
        "save_predictions_to_csv(output, final_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_predictions2 = predict_and_merge2(best_xgb_model, normalized_test_a, normalized_test_b)\n",
        "save_predictions_to_csv(\"numsum_preds.csv\", final_predictions2)\n",
        "\n",
        "\n",
        "#23;17     40                        ONLY WRONG ONE, 17 est prédit comme 18.\n",
        "#18;23     41\n",
        "#13;20     33\n",
        "#12;11     23\n",
        "#19;14     33\n",
        "\n",
        "##Testing with testcustom.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CNN Basic Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aklxAC28-zBr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "687/687 [==============================] - 8s 10ms/step - loss: 0.4676 - accuracy: 0.8622 - val_loss: 0.0474 - val_accuracy: 0.9940\n",
            "Epoch 2/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0450 - accuracy: 0.9863 - val_loss: 0.0054 - val_accuracy: 0.9989\n",
            "Epoch 3/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0271 - accuracy: 0.9921 - val_loss: 0.0125 - val_accuracy: 0.9964\n",
            "Epoch 4/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0509 - accuracy: 0.9829 - val_loss: 0.0042 - val_accuracy: 0.9984\n",
            "Epoch 5/50\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0207 - accuracy: 0.9936 - val_loss: 0.0011 - val_accuracy: 0.9998\n",
            "Epoch 6/50\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0336 - accuracy: 0.9890 - val_loss: 0.0410 - val_accuracy: 0.9856\n",
            "Epoch 7/50\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0242 - accuracy: 0.9919 - val_loss: 0.0725 - val_accuracy: 0.9792\n",
            "Epoch 8/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0213 - accuracy: 0.9925 - val_loss: 0.0013 - val_accuracy: 0.9995\n",
            "Epoch 9/50\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0220 - accuracy: 0.9933 - val_loss: 0.0044 - val_accuracy: 0.9982\n",
            "Epoch 10/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0163 - accuracy: 0.9945 - val_loss: 0.1050 - val_accuracy: 0.9710\n",
            "Epoch 11/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0222 - accuracy: 0.9938 - val_loss: 0.0048 - val_accuracy: 0.9984\n",
            "Epoch 12/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0210 - accuracy: 0.9936 - val_loss: 0.0085 - val_accuracy: 0.9969\n",
            "Epoch 13/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0133 - accuracy: 0.9959 - val_loss: 0.0020 - val_accuracy: 0.9991\n",
            "Epoch 14/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0207 - accuracy: 0.9942 - val_loss: 0.0061 - val_accuracy: 0.9984\n",
            "Epoch 15/50\n",
            "687/687 [==============================] - 7s 9ms/step - loss: 0.0127 - accuracy: 0.9961 - val_loss: 1.6269 - val_accuracy: 0.8184\n",
            "Epoch 16/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0116 - accuracy: 0.9968 - val_loss: 5.9870e-04 - val_accuracy: 0.9998\n",
            "Epoch 17/50\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0204 - accuracy: 0.9949 - val_loss: 0.0391 - val_accuracy: 0.9922\n",
            "Epoch 18/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0107 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9995\n",
            "Epoch 19/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0152 - accuracy: 0.9959 - val_loss: 0.0030 - val_accuracy: 0.9989\n",
            "Epoch 20/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0144 - accuracy: 0.9957 - val_loss: 2.6439e-06 - val_accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0133 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9993\n",
            "Epoch 22/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0194 - accuracy: 0.9949 - val_loss: 0.0550 - val_accuracy: 0.9838\n",
            "Epoch 23/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0122 - accuracy: 0.9961 - val_loss: 0.0275 - val_accuracy: 0.9954\n",
            "Epoch 24/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0127 - accuracy: 0.9967 - val_loss: 0.5226 - val_accuracy: 0.9374\n",
            "Epoch 25/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0088 - accuracy: 0.9974 - val_loss: 1.5202e-05 - val_accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0124 - accuracy: 0.9970 - val_loss: 1.5839e-04 - val_accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0103 - accuracy: 0.9969 - val_loss: 3.1324e-05 - val_accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0139 - accuracy: 0.9969 - val_loss: 0.0018 - val_accuracy: 0.9987\n",
            "Epoch 29/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0148 - accuracy: 0.9967 - val_loss: 0.0800 - val_accuracy: 0.9840\n",
            "Epoch 30/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0145 - accuracy: 0.9964 - val_loss: 0.0124 - val_accuracy: 0.9958\n",
            "Epoch 31/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0098 - accuracy: 0.9974 - val_loss: 3.9794e-04 - val_accuracy: 0.9998\n",
            "Epoch 32/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0090 - accuracy: 0.9976 - val_loss: 0.0015 - val_accuracy: 0.9995\n",
            "Epoch 33/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0083 - accuracy: 0.9975 - val_loss: 0.0046 - val_accuracy: 0.9984\n",
            "Epoch 34/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0109 - accuracy: 0.9969 - val_loss: 0.0014 - val_accuracy: 0.9995\n",
            "Epoch 35/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0089 - accuracy: 0.9978 - val_loss: 9.2322e-05 - val_accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0083 - accuracy: 0.9980 - val_loss: 2.0183e-07 - val_accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0091 - accuracy: 0.9975 - val_loss: 0.0458 - val_accuracy: 0.9903\n",
            "Epoch 38/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0106 - accuracy: 0.9970 - val_loss: 2.2873e-05 - val_accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0098 - accuracy: 0.9974 - val_loss: 1.5918e-05 - val_accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0056 - accuracy: 0.9985 - val_loss: 8.8213e-07 - val_accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "687/687 [==============================] - 7s 9ms/step - loss: 0.0093 - accuracy: 0.9979 - val_loss: 7.2603e-04 - val_accuracy: 0.9998\n",
            "Epoch 42/50\n",
            "687/687 [==============================] - 7s 9ms/step - loss: 0.0107 - accuracy: 0.9978 - val_loss: 0.0022 - val_accuracy: 0.9993\n",
            "Epoch 43/50\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0083 - accuracy: 0.9982 - val_loss: 3.7536e-08 - val_accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0085 - accuracy: 0.9983 - val_loss: 3.6018e-05 - val_accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0101 - accuracy: 0.9975 - val_loss: 0.0033 - val_accuracy: 0.9989\n",
            "Epoch 46/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0098 - accuracy: 0.9973 - val_loss: 1.9139e-05 - val_accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0070 - accuracy: 0.9982 - val_loss: 1.1548e-05 - val_accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0114 - accuracy: 0.9973 - val_loss: 0.0189 - val_accuracy: 0.9954\n",
            "Epoch 49/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0111 - accuracy: 0.9975 - val_loss: 0.0015 - val_accuracy: 0.9998\n",
            "Epoch 50/50\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0098 - accuracy: 0.9974 - val_loss: 3.4188e-05 - val_accuracy: 1.0000\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "94/94 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "# Setting a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "split_ratio = 0.8\n",
        "\n",
        "###################### -- HELPER FUNCTIONS -- ######################\n",
        "\n",
        "def separate_test_sets(file_path):\n",
        "    test_data = pd.read_csv(file_path)\n",
        "    test_a_columns = [col for col in test_data.columns if 'pixel_a' in col]\n",
        "    test_b_columns = [col for col in test_data.columns if 'pixel_b' in col]\n",
        "    test_a = test_data[test_a_columns]\n",
        "    test_b = test_data[test_b_columns]\n",
        "    test_a.columns = [col.replace('_a', '') for col in test_a.columns]\n",
        "    test_b.columns = [col.replace('_b', '') for col in test_b.columns]\n",
        "    return test_a, test_b\n",
        "\n",
        "def normalize_ascii_sum(ascii_sum):\n",
        "    while ascii_sum > 122:  # 'z' is ASCII 122\n",
        "        ascii_sum -= 65  # 122 ('z') - 65 ('A') + 1\n",
        "    return int(ascii_sum)\n",
        "\n",
        "def save_predictions_to_csv(filename, predictions):\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(\"id,label\\n\")\n",
        "        for id, label in predictions:\n",
        "            file.write(f\"{id},{label}\\n\")\n",
        "\n",
        "###################### -- CNN Implementation -- ######################\n",
        "\n",
        "def build_cnn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        BatchNormalization(),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', \n",
        "                  loss='sparse_categorical_crossentropy', \n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "###################### -- Data Handling -- ######################\n",
        "\n",
        "# Load your dataset here\n",
        "mnist_sign_train = pd.read_csv('sign_mnist_train.csv')\n",
        "mnist_sign_train['label'] = mnist_sign_train['label'].replace(24, 9)  # Replace 24 with 9\n",
        "mnist_sign_train = mnist_sign_train.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Prepare the data\n",
        "labels = mnist_sign_train['label'].values\n",
        "features = mnist_sign_train.drop('label', axis=1).values\n",
        "\n",
        "# Normalize features\n",
        "features_normalized = features / 255.0\n",
        "\n",
        "# Split data into training and validation sets\n",
        "split_index = int(split_ratio * len(features_normalized))\n",
        "train_data, validation_data = features_normalized[:split_index], features_normalized[split_index:]\n",
        "train_labels, validation_labels = labels[:split_index], labels[split_index:]\n",
        "\n",
        "# Reshape data for CNN input\n",
        "train_images = train_data.reshape((-1, 28, 28, 1))\n",
        "validation_images = validation_data.reshape((-1, 28, 28, 1))\n",
        "\n",
        "###################### -- CNN Training -- ######################\n",
        "\n",
        "# Define and build CNN model\n",
        "input_shape = (28, 28, 1)\n",
        "num_classes = len(np.unique(train_labels))\n",
        "cnn_model = build_cnn_model(input_shape, num_classes)\n",
        "\n",
        "# Train the CNN model\n",
        "cnn_model.fit(train_images, train_labels, epochs=50, validation_data=(validation_images, validation_labels))\n",
        "\n",
        "###################### -- Predict and Save to CSV -- ######################\n",
        "\n",
        "# Prepare test data\n",
        "test_a, test_b = separate_test_sets('test.csv')\n",
        "normalized_test_a = test_a.values.reshape((-1, 28, 28, 1)) / 255.0\n",
        "normalized_test_b = test_b.values.reshape((-1, 28, 28, 1)) / 255.0\n",
        "\n",
        "# Predict on test data\n",
        "preds_a = cnn_model.predict(normalized_test_a)\n",
        "preds_b = cnn_model.predict(normalized_test_b)\n",
        "\n",
        "# Convert predictions to labels\n",
        "preds_a_labels = np.argmax(preds_a, axis=1)\n",
        "preds_b_labels = np.argmax(preds_b, axis=1)\n",
        "\n",
        "# Replacing 9 back to 24 if needed, and ASCII manipulation\n",
        "merged_predictions = []\n",
        "for i in range(len(preds_a_labels)):\n",
        "    decoded_a = preds_a_labels[i] if preds_a_labels[i] != 9 else 24\n",
        "    decoded_b = preds_b_labels[i] if preds_b_labels[i] != 9 else 24\n",
        "\n",
        "    ascii_a = decoded_a + 65\n",
        "    ascii_b = decoded_b + 65\n",
        "    sum_pred = normalize_ascii_sum(ascii_a + ascii_b)\n",
        "    merged_predictions.append((i, chr(sum_pred)))\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_predictions_to_csv(\"cnnn_predictions.csv\", merged_predictions)\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CNN with K-Fold Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "687/687 [==============================] - 8s 10ms/step - loss: 0.4864 - accuracy: 0.8595 - val_loss: 0.0554 - val_accuracy: 0.9920\n",
            "Epoch 2/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0471 - accuracy: 0.9860 - val_loss: 0.0105 - val_accuracy: 0.9982\n",
            "Epoch 3/10\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0316 - accuracy: 0.9907 - val_loss: 0.0037 - val_accuracy: 0.9993\n",
            "Epoch 4/10\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0329 - accuracy: 0.9892 - val_loss: 0.0334 - val_accuracy: 0.9891\n",
            "Epoch 5/10\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0256 - accuracy: 0.9921 - val_loss: 0.1305 - val_accuracy: 0.9536\n",
            "Epoch 6/10\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0280 - accuracy: 0.9903 - val_loss: 0.1069 - val_accuracy: 0.9679\n",
            "Epoch 7/10\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0257 - accuracy: 0.9910 - val_loss: 0.0057 - val_accuracy: 0.9978\n",
            "Epoch 8/10\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0150 - accuracy: 0.9948 - val_loss: 6.4403e-04 - val_accuracy: 0.9998\n",
            "Epoch 9/10\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0217 - accuracy: 0.9927 - val_loss: 0.0200 - val_accuracy: 0.9945\n",
            "Epoch 10/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0233 - accuracy: 0.9929 - val_loss: 0.0025 - val_accuracy: 0.9989\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.4854 - accuracy: 0.8585 - val_loss: 0.0967 - val_accuracy: 0.9741\n",
            "Epoch 2/10\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0401 - accuracy: 0.9886 - val_loss: 0.0042 - val_accuracy: 0.9991\n",
            "Epoch 3/10\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0285 - accuracy: 0.9913 - val_loss: 0.0062 - val_accuracy: 0.9987\n",
            "Epoch 4/10\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0344 - accuracy: 0.9884 - val_loss: 0.0428 - val_accuracy: 0.9820\n",
            "Epoch 5/10\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0310 - accuracy: 0.9904 - val_loss: 0.0111 - val_accuracy: 0.9964\n",
            "Epoch 6/10\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0359 - accuracy: 0.9884 - val_loss: 0.0086 - val_accuracy: 0.9971\n",
            "Epoch 7/10\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0152 - accuracy: 0.9950 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0243 - accuracy: 0.9919 - val_loss: 0.0258 - val_accuracy: 0.9925\n",
            "Epoch 9/10\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0167 - accuracy: 0.9943 - val_loss: 0.0295 - val_accuracy: 0.9925\n",
            "Epoch 10/10\n",
            "687/687 [==============================] - 6s 9ms/step - loss: 0.0280 - accuracy: 0.9915 - val_loss: 0.0550 - val_accuracy: 0.9856\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "687/687 [==============================] - 9s 12ms/step - loss: 0.4880 - accuracy: 0.8542 - val_loss: 0.0243 - val_accuracy: 0.9993\n",
            "Epoch 2/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0412 - accuracy: 0.9890 - val_loss: 0.0139 - val_accuracy: 0.9962\n",
            "Epoch 3/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0350 - accuracy: 0.9882 - val_loss: 0.0040 - val_accuracy: 0.9991\n",
            "Epoch 4/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0328 - accuracy: 0.9893 - val_loss: 0.0036 - val_accuracy: 0.9996\n",
            "Epoch 5/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0233 - accuracy: 0.9929 - val_loss: 0.0252 - val_accuracy: 0.9925\n",
            "Epoch 6/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0346 - accuracy: 0.9887 - val_loss: 0.0363 - val_accuracy: 0.9887\n",
            "Epoch 7/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0207 - accuracy: 0.9930 - val_loss: 0.0289 - val_accuracy: 0.9907\n",
            "Epoch 8/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0243 - accuracy: 0.9924 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.0050 - val_accuracy: 0.9978\n",
            "Epoch 10/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0182 - accuracy: 0.9946 - val_loss: 0.0360 - val_accuracy: 0.9874\n",
            "Training for fold 4 ...\n",
            "Epoch 1/10\n",
            "687/687 [==============================] - 8s 11ms/step - loss: 0.4874 - accuracy: 0.8537 - val_loss: 0.0379 - val_accuracy: 0.9956\n",
            "Epoch 2/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0523 - accuracy: 0.9849 - val_loss: 0.0169 - val_accuracy: 0.9967\n",
            "Epoch 3/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0301 - accuracy: 0.9910 - val_loss: 0.0029 - val_accuracy: 0.9996\n",
            "Epoch 4/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0345 - accuracy: 0.9881 - val_loss: 0.0065 - val_accuracy: 0.9975\n",
            "Epoch 5/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0291 - accuracy: 0.9912 - val_loss: 0.0276 - val_accuracy: 0.9894\n",
            "Epoch 6/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0259 - accuracy: 0.9910 - val_loss: 0.0047 - val_accuracy: 0.9985\n",
            "Epoch 7/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0248 - accuracy: 0.9916 - val_loss: 0.0267 - val_accuracy: 0.9913\n",
            "Epoch 8/10\n",
            "687/687 [==============================] - 8s 11ms/step - loss: 0.0277 - accuracy: 0.9907 - val_loss: 0.0316 - val_accuracy: 0.9891\n",
            "Epoch 9/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0160 - accuracy: 0.9951 - val_loss: 5.2418e-04 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "687/687 [==============================] - 8s 11ms/step - loss: 0.0244 - accuracy: 0.9923 - val_loss: 0.0118 - val_accuracy: 0.9964\n",
            "Training for fold 5 ...\n",
            "Epoch 1/10\n",
            "687/687 [==============================] - 8s 10ms/step - loss: 0.4888 - accuracy: 0.8547 - val_loss: 0.1600 - val_accuracy: 0.9583\n",
            "Epoch 2/10\n",
            "687/687 [==============================] - 7s 10ms/step - loss: 0.0439 - accuracy: 0.9875 - val_loss: 0.0140 - val_accuracy: 0.9964\n",
            "Epoch 3/10\n",
            "687/687 [==============================] - 9s 13ms/step - loss: 0.0354 - accuracy: 0.9887 - val_loss: 0.0038 - val_accuracy: 0.9996\n",
            "Epoch 4/10\n",
            "687/687 [==============================] - 8s 12ms/step - loss: 0.0264 - accuracy: 0.9918 - val_loss: 1.4097 - val_accuracy: 0.7423\n",
            "Epoch 5/10\n",
            "687/687 [==============================] - 8s 11ms/step - loss: 0.0414 - accuracy: 0.9854 - val_loss: 0.0077 - val_accuracy: 0.9982\n",
            "Epoch 6/10\n",
            "687/687 [==============================] - 8s 11ms/step - loss: 0.0299 - accuracy: 0.9904 - val_loss: 0.0018 - val_accuracy: 0.9995\n",
            "Epoch 7/10\n",
            "687/687 [==============================] - 8s 11ms/step - loss: 0.0210 - accuracy: 0.9925 - val_loss: 0.0035 - val_accuracy: 0.9982\n",
            "Epoch 8/10\n",
            "687/687 [==============================] - 7s 11ms/step - loss: 0.0222 - accuracy: 0.9931 - val_loss: 0.0043 - val_accuracy: 0.9985\n",
            "Epoch 9/10\n",
            "687/687 [==============================] - 7s 11ms/step - loss: 0.0220 - accuracy: 0.9929 - val_loss: 0.0387 - val_accuracy: 0.9878\n",
            "Epoch 10/10\n",
            "687/687 [==============================] - 8s 11ms/step - loss: 0.0180 - accuracy: 0.9941 - val_loss: 8.7796e-04 - val_accuracy: 0.9996\n",
            "94/94 [==============================] - 0s 3ms/step\n",
            "94/94 [==============================] - 0s 3ms/step\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Setting a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "split_ratio = 0.8\n",
        "\n",
        "###################### -- HELPER FUNCTIONS -- ######################\n",
        "\n",
        "def separate_test_sets(file_path):\n",
        "    test_data = pd.read_csv(file_path)\n",
        "    test_a_columns = [col for col in test_data.columns if 'pixel_a' in col]\n",
        "    test_b_columns = [col for col in test_data.columns if 'pixel_b' in col]\n",
        "    test_a = test_data[test_a_columns]\n",
        "    test_b = test_data[test_b_columns]\n",
        "    test_a.columns = [col.replace('_a', '') for col in test_a.columns]\n",
        "    test_b.columns = [col.replace('_b', '') for col in test_b.columns]\n",
        "    return test_a, test_b\n",
        "\n",
        "def normalize_ascii_sum(ascii_sum):\n",
        "    while ascii_sum > 122:  # 'z' is ASCII 122\n",
        "        ascii_sum -= 65  # 122 ('z') - 65 ('A') + 1\n",
        "    return int(ascii_sum)\n",
        "\n",
        "def save_predictions_to_csv(filename, predictions):\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(\"id,label\\n\")\n",
        "        for id, label in predictions:\n",
        "            file.write(f\"{id},{label}\\n\")\n",
        "\n",
        "###################### -- CNN Implementation -- ######################\n",
        "\n",
        "def build_cnn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        BatchNormalization(),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', \n",
        "                  loss='sparse_categorical_crossentropy', \n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "###################### -- Data Handling and CNN Training with Cross-Validation -- ######################\n",
        "\n",
        "# Load your dataset here\n",
        "mnist_sign_train = pd.read_csv('sign_mnist_train.csv')\n",
        "mnist_sign_train['label'] = mnist_sign_train['label'].replace(24, 9)  # Replace 24 with 9\n",
        "mnist_sign_train = mnist_sign_train.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Prepare the data\n",
        "labels = mnist_sign_train['label'].values\n",
        "features = mnist_sign_train.drop('label', axis=1).values\n",
        "\n",
        "# Normalize features\n",
        "features_normalized = features / 255.0\n",
        "images = features_normalized.reshape((-1, 28, 28, 1))\n",
        "\n",
        "# Define k-fold cross-validation\n",
        "kfold = KFold(n_splits=20, shuffle=True, random_state=42)\n",
        "fold_no = 1\n",
        "num_classes = len(np.unique(labels))\n",
        "\n",
        "for train, test in kfold.split(images, labels):\n",
        "    train_images, test_images = images[train], images[test]\n",
        "    train_labels, test_labels = labels[train], labels[test]\n",
        "\n",
        "    # Define and build CNN model\n",
        "    cnn_model = build_cnn_model(input_shape=(28, 28, 1), num_classes=num_classes)\n",
        "\n",
        "    # Train the CNN model\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "    cnn_model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n",
        "\n",
        "    # Increase fold number\n",
        "    fold_no += 1\n",
        "\n",
        "# Save the final model if needed\n",
        "# cnn_model.save('path_to_my_model.h5')\n",
        "\n",
        "###################### -- Predict and Save to CSV -- ######################\n",
        "\n",
        "# Prepare test data\n",
        "test_a, test_b = separate_test_sets('test.csv')\n",
        "normalized_test_a = test_a.values.reshape((-1, 28, 28, 1)) / 255.0\n",
        "normalized_test_b = test_b.values.reshape((-1, 28, 28, 1)) / 255.0\n",
        "\n",
        "# Predict on test data\n",
        "preds_a = cnn_model.predict(normalized_test_a)\n",
        "preds_b = cnn_model.predict(normalized_test_b)\n",
        "\n",
        "# Convert predictions to labels\n",
        "preds_a_labels = np.argmax(preds_a, axis=1)\n",
        "preds_b_labels = np.argmax(preds_b, axis=1)\n",
        "\n",
        "# Replacing 9 back to 24 if needed, and ASCII manipulation\n",
        "merged_predictions = []\n",
        "for i in range(len(preds_a_labels)):\n",
        "    decoded_a = preds_a_labels[i] if preds_a_labels[i] != 9 else 24\n",
        "    decoded_b = preds_b_labels[i] if preds_b_labels[i] != 9 else 24\n",
        "\n",
        "    ascii_a = decoded_a + 65\n",
        "    ascii_b = decoded_b + 65\n",
        "    sum_pred = normalize_ascii_sum(ascii_a + ascii_b)\n",
        "    merged_predictions.append((i, chr(sum_pred)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_predictions_to_csv(\"cnn_fold__predictions.csv\", merged_predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CNN adapted from https://github.com/brendanartley/Medium-Article-Code/blob/main/code/mnist-keras-cnn-99-6.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "171/428 [==========>...................] - ETA: 22s - loss: 1.6895 - accuracy: 0.5134"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# Setting a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "###################### -- HELPER FUNCTIONS -- ######################\n",
        "\n",
        "def separate_test_sets(file_path):\n",
        "    test_data = pd.read_csv(file_path)\n",
        "    test_a_columns = [col for col in test_data.columns if 'pixel_a' in col]\n",
        "    test_b_columns = [col for col in test_data.columns if 'pixel_b' in col]\n",
        "    test_a = test_data[test_a_columns]\n",
        "    test_b = test_data[test_b_columns]\n",
        "    test_a.columns = [col.replace('_a', '') for col in test_a.columns]\n",
        "    test_b.columns = [col.replace('_b', '') for col in test_b.columns]\n",
        "    return test_a, test_b\n",
        "\n",
        "def normalize_ascii_sum(ascii_sum):\n",
        "    while ascii_sum > 122:  # 'z' is ASCII 122\n",
        "        ascii_sum -= 65  # 122 ('z') - 65 ('A') + 1\n",
        "    return int(ascii_sum)\n",
        "\n",
        "def save_predictions_to_csv(filename, predictions):\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(\"id,label\\n\")\n",
        "        for id, label in predictions:\n",
        "            file.write(f\"{id},{label}\\n\")\n",
        "\n",
        "###################### -- CNN Implementation -- ######################\n",
        "\n",
        "def build_cnn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
        "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "###################### -- Data Handling and CNN Training -- ######################\n",
        "\n",
        "# Load your dataset here\n",
        "mnist_sign_train = pd.read_csv('sign_mnist_train.csv')\n",
        "mnist_sign_train['label'] = mnist_sign_train['label'].replace(24, 9)  # Replace 24 with 9\n",
        "mnist_sign_train = mnist_sign_train.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Prepare the data\n",
        "labels = mnist_sign_train['label'].values\n",
        "features = mnist_sign_train.drop('label', axis=1).values\n",
        "\n",
        "# Normalize features and one-hot encode the labels\n",
        "features_normalized = features / 255.0\n",
        "images = features_normalized.reshape((-1, 28, 28, 1))\n",
        "labels_encoded = tf.keras.utils.to_categorical(labels, num_classes=25)\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        ")\n",
        "\n",
        "# Define and build CNN model\n",
        "input_shape = (28, 28, 1)\n",
        "num_classes = 25  # 24 classes + 1 for the replaced label\n",
        "cnn_model = build_cnn_model(input_shape, num_classes)\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "reduce_lr = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n",
        "\n",
        "# Train the CNN model\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "history = cnn_model.fit(\n",
        "    datagen.flow(images, labels_encoded, batch_size=batch_size),\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=images.shape[0] // batch_size,\n",
        "    callbacks=[reduce_lr]\n",
        ")\n",
        "\n",
        "\n",
        "###################### -- Predict and Save to CSV -- ######################\n",
        "\n",
        "# Prepare test data\n",
        "test_a, test_b = separate_test_sets('test.csv')\n",
        "normalized_test_a = test_a.values.reshape((-1, 28, 28, 1)) / 255.0\n",
        "normalized_test_b = test_b.values.reshape((-1, 28, 28, 1)) / 255.0\n",
        "\n",
        "# Predict on test data\n",
        "preds_a = cnn_model.predict(normalized_test_a)\n",
        "preds_b = cnn_model.predict(normalized_test_b)\n",
        "\n",
        "# Convert predictions to labels\n",
        "preds_a_labels = np.argmax(preds_a, axis=1)\n",
        "preds_b_labels = np.argmax(preds_b, axis=1)\n",
        "\n",
        "# Replacing 9 back to 24 if needed, and ASCII manipulation\n",
        "merged_predictions = []\n",
        "for i in range(len(preds_a_labels)):\n",
        "    decoded_a = preds_a_labels[i] if preds_a_labels[i] != 9 else 24\n",
        "    decoded_b = preds_b_labels[i] if preds_b_labels[i] != 9 else 24\n",
        "\n",
        "    ascii_a = decoded_a + 65\n",
        "    ascii_b = decoded_b + 65\n",
        "    sum_pred = normalize_ascii_sum(ascii_a + ascii_b)\n",
        "    merged_predictions.append((i, chr(sum_pred)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save predictions to CSV file\n",
        "save_predictions_to_csv(\"cnn_predictions.csv\", merged_predictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
