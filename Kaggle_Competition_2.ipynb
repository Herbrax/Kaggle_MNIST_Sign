{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_9DAM7b9VdT",
        "outputId": "dfd3c9d3-c2c2-4a03-e2da-29550ee90063"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "105\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "\n",
        "decoded_a = 23\n",
        "decoded_b = 17\n",
        "ascii_a = decoded_a + 65\n",
        "ascii_b = decoded_b + 65\n",
        "sum_pred = ascii_a + ascii_b\n",
        "while sum_pred > 122:\n",
        "  sum_pred -= 65\n",
        "print(sum_pred)\n",
        "print(chr(sum_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9f6AJypx9VdU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "\n",
        "# Setting a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "split_ratio = 0.8\n",
        "train_file_path = 'https://raw.githubusercontent.com/Herbrax/Kaggle_MNIST_Sign/main/sign_mnist_train2.csv'\n",
        "test_file_path = 'https://raw.githubusercontent.com/Herbrax/Kaggle_MNIST_Sign/main/test2.csv'\n",
        "\n",
        "###################### -- HELPER FUNCTIONS -- ######################\n",
        "\n",
        "def separate_test_sets(file_path):\n",
        "    test_data = pd.read_csv(file_path)\n",
        "    test_a_columns = [col for col in test_data.columns if 'pixel_a' in col]\n",
        "    test_b_columns = [col for col in test_data.columns if 'pixel_b' in col]\n",
        "    test_a = test_data[test_a_columns]\n",
        "    test_b = test_data[test_b_columns]\n",
        "    test_a.columns = [col.replace('_a', '') for col in test_a.columns]\n",
        "    test_b.columns = [col.replace('_b', '') for col in test_b.columns]\n",
        "    return test_a, test_b\n",
        "\n",
        "def predict_and_merge(model, test_a, test_b):\n",
        "    preds_a = model.compute_predictions(test_a)\n",
        "    preds_b = model.compute_predictions(test_b)\n",
        "    merged_predictions = []\n",
        "    for i in range(len(preds_a)):\n",
        "        # Replacing 9 back to 24 if needed\n",
        "        decoded_a = preds_a[i] if preds_a[i] != 9 else 24\n",
        "        decoded_b = preds_b[i] if preds_b[i] != 9 else 24\n",
        "\n",
        "        ascii_a = decoded_a + 65\n",
        "        ascii_b = decoded_b + 65\n",
        "        sum_pred = ascii_a + ascii_b\n",
        "        while sum_pred > 122:\n",
        "            sum_pred -= 65\n",
        "        merged_predictions.append((i, chr(int(sum_pred))))\n",
        "    return merged_predictions\n",
        "\n",
        "def predict_and_merge2(model, test_a, test_b):\n",
        "    preds_a = model.compute_predictions(test_a)\n",
        "    preds_b = model.compute_predictions(test_b)\n",
        "    merged_predictions = []\n",
        "    for i in range(len(preds_a)):\n",
        "        decoded_a = preds_a[i] if preds_a[i] != 9 else 24\n",
        "        decoded_b = preds_b[i] if preds_b[i] != 9 else 24\n",
        "        sum_pred = decoded_a + decoded_b\n",
        "        print(decoded_a,\";\",decoded_b,\"----------------------\",sum_pred)\n",
        "        merged_predictions.append((i, sum_pred))\n",
        "    return merged_predictions\n",
        "\n",
        "def save_predictions_to_csv(filename, predictions):\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(\"id,label\\n\")\n",
        "        for id, label in predictions:\n",
        "            file.write(f\"{id},{label}\\n\")\n",
        "\n",
        "def plot_results_XGB(results):\n",
        "    learning_rates = [x[0] for x in results]\n",
        "    max_depths = [x[1] for x in results]\n",
        "    accuracies = [x[2] for x in results]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for lr in set(learning_rates):\n",
        "        specific_lr_depths = [depth for depth, l_rate in zip(max_depths, learning_rates) if l_rate == lr]\n",
        "        specific_acc = [acc for acc, l_rate in zip(accuracies, learning_rates) if l_rate == lr]\n",
        "        plt.plot(specific_lr_depths, specific_acc, label=f'Learning Rate {lr}')\n",
        "    plt.title('Accuracy for different max depths and learning rates')\n",
        "    plt.xlabel('Max Depth')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "###################### -- Data Handling -- ######################\n",
        "\n",
        "mnist_sign_train = pd.read_csv(train_file_path)\n",
        "mnist_sign_train['label'] = mnist_sign_train['label'].replace(24, 9)\n",
        "mnist_sign_train = mnist_sign_train.sample(frac=1).reset_index(drop=True)\n",
        "labels = mnist_sign_train['label'].values\n",
        "features = mnist_sign_train.drop('label', axis=1).values\n",
        "\n",
        "features_normalized = features / 255.0\n",
        "split_index = int(split_ratio * len(features_normalized))\n",
        "train_data, validation_data = features_normalized[:split_index], features_normalized[split_index:]\n",
        "train_labels, validation_labels = labels[:split_index], labels[split_index:]\n",
        "\n",
        "train_mean = train_data.mean(axis=0)\n",
        "train_std = train_data.std(axis=0)\n",
        "train_data_normalized = (train_data - train_mean) / train_std\n",
        "validation_data_normalized = (validation_data - train_mean) / train_std\n",
        "\n",
        "test_a, test_b = separate_test_sets(test_file_path)\n",
        "normalized_test_a = (test_a.values / 255.0 - train_mean) / train_std\n",
        "normalized_test_b = (test_b.values / 255.0 - train_mean) / train_std\n",
        "\n",
        "###################### -- XGBOOST Implementation -- ######################\n",
        "\n",
        "class XGBoostClassifier:\n",
        "    def __init__(self, max_depth, eta, num_class):\n",
        "        self.params = {\n",
        "            'objective': 'multi:softmax',\n",
        "            'num_class': num_class,\n",
        "            'booster': 'gbtree',\n",
        "            'eval_metric': 'merror',\n",
        "            'eta': eta,\n",
        "            'max_depth': max_depth,\n",
        "        }\n",
        "\n",
        "    def train(self, train_data, train_labels, validation_data, validation_labels):\n",
        "        dtrain = xgb.DMatrix(train_data, label=train_labels)\n",
        "        dval = xgb.DMatrix(validation_data, label=validation_labels)\n",
        "        watchlist = [(dtrain, 'train'), (dval, 'validation')]\n",
        "        self.model = xgb.train(self.params, dtrain, num_boost_round=200, evals=watchlist, early_stopping_rounds=20, verbose_eval=False)\n",
        "\n",
        "    def compute_predictions(self, data):\n",
        "        ddata = xgb.DMatrix(data)\n",
        "        return self.model.predict(ddata)\n",
        "\n",
        "    def compute_accuracy(self, preds, labels):\n",
        "        return np.mean(preds == labels)\n",
        "\n",
        "###################### -- TRAINING -- ######################\n",
        "\n",
        "def startXGBOOST(train_data_normalized, train_labels, validation_data_normalized, validation_labels):\n",
        "    learning_rates = [0.1]\n",
        "    max_depths = [3]\n",
        "    results_XGB = []\n",
        "    best_acc_xgb = 0\n",
        "    best_lr = None\n",
        "    best_depth = None\n",
        "    num_class = len(np.unique(train_labels))\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        for depth in max_depths:\n",
        "            xgb_model = XGBoostClassifier(max_depth=depth, eta=lr, num_class=num_class)\n",
        "            xgb_model.train(train_data_normalized, train_labels, validation_data_normalized, validation_labels)\n",
        "            val_preds = xgb_model.compute_predictions(validation_data_normalized)\n",
        "            acc = xgb_model.compute_accuracy(val_preds, validation_labels)\n",
        "            if acc > best_acc_xgb:\n",
        "                best_acc_xgb = acc\n",
        "                best_lr = lr\n",
        "                best_depth = depth\n",
        "            results_XGB.append((lr, depth, acc))\n",
        "\n",
        "    #plot_results_XGB(results_XGB)\n",
        "\n",
        "    best_xgb_model = XGBoostClassifier(max_depth=best_depth, eta=best_lr, num_class=num_class)\n",
        "    best_xgb_model.train(train_data_normalized, train_labels, validation_data_normalized, validation_labels)\n",
        "\n",
        "    return best_xgb_model\n",
        "\n",
        "#best_xgb_model = startXGBOOST(train_data_normalized, train_labels, validation_data_normalized, validation_labels)\n",
        "\n",
        "#final_predictions = predict_and_merge(best_xgb_model, normalized_test_a, normalized_test_b)\n",
        "\n",
        "#save_merged_predictions_to_csv(\"merged_predictions.csv\", final_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "akwod-Qh9VdU",
        "outputId": "19d5c8c0-353a-4566-c43f-8abc6aa87055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23.0 ; 18.0 ---------------------- 41.0\n",
            "18.0 ; 23.0 ---------------------- 41.0\n",
            "13.0 ; 20.0 ---------------------- 33.0\n",
            "12.0 ; 11.0 ---------------------- 23.0\n",
            "19.0 ; 14.0 ---------------------- 33.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n23;17     40                        ONLY WRONG ONE, 17 est prédit comme 18.\\n18;23     41\\n13;20     33\\n12;11     23\\n19;14     33\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "best_xgb_model = startXGBOOST(train_data_normalized, train_labels, validation_data_normalized, validation_labels)\n",
        "\n",
        "final_predictions = predict_and_merge(best_xgb_model, normalized_test_a, normalized_test_b)\n",
        "final_predictions2 = predict_and_merge2(best_xgb_model, normalized_test_a, normalized_test_b)\n",
        "\n",
        "save_predictions_to_csv(\"ascii_preds.csv\", final_predictions)\n",
        "save_predictions_to_csv(\"numsum_preds.csv\", final_predictions2)\n",
        "\n",
        "\n",
        "#23;17     40                        ONLY WRONG ONE, 17 est prédit comme 18.\n",
        "#18;23     41\n",
        "#13;20     33\n",
        "#12;11     23\n",
        "#19;14     33\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aklxAC28-zBr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}