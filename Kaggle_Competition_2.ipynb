{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mLmacOFkua3Y"
      },
      "outputs": [],
      "source": [
        "## Kaggle Competition 2 - Simo Hakim - 20096040\n",
        "\n",
        "ratio = 0.8\n",
        "###################### -- Imports -- ######################\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import LinearSVC\n",
        "import xgboost as xgb\n",
        "\n",
        "import threading\n",
        "np.random.seed(42)\n",
        "np.set_printoptions(precision=2, suppress=True)\n",
        "###################### -- Helper Functions -- ######################\n",
        "\n",
        "### Pour sauver mes prédictions :\n",
        "def save_predictions_to_csv(filename, ids, predictions):\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(\"id,label\\n\")\n",
        "        for id, pred in zip(ids, predictions):\n",
        "            file.write(f\"{id},{pred}\\n\")\n",
        "\n",
        "### Pour plotter mes resultats :\n",
        "def plot_results_XGB(results):\n",
        "    learning_rates = [x[0] for x in results]\n",
        "    max_depths = [x[1] for x in results]\n",
        "    accuracies = [x[2] for x in results]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for lr in set(learning_rates):\n",
        "        specific_lr_depths = [depth for depth, l_rate in zip(max_depths, learning_rates) if l_rate == lr]\n",
        "        specific_acc = [acc for acc, l_rate in zip(accuracies, learning_rates) if l_rate == lr]\n",
        "        plt.plot(specific_lr_depths, specific_acc, label=f'Learning Rate {lr}')\n",
        "\n",
        "    plt.title(f'Accuracy for different max depths and learning rates')\n",
        "    plt.xlabel('Max Depth')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "###################### -- Data Handling -- ######################\n",
        "\n",
        "# Importation des données\n",
        "train = np.genfromtxt('sign_mnist_train.csv', delimiter=',', skip_header=1)\n",
        "test = np.genfromtxt('test.csv', delimiter=',', skip_header=1)\n",
        "\n",
        "# Préparation des labels d'entraînement\n",
        "train_labels = train[:, 0]  # Première colonne pour les labels\n",
        "train_pixels = np.delete(train, 0, axis=1)  # Suppression de la colonne des labels\n",
        "\n",
        "# Conversion en labels one-hot pour la régression logistique\n",
        "unique_labels = np.unique(train_labels)\n",
        "train_labels_one_hot = np.zeros((train.shape[0], len(unique_labels)))\n",
        "for i, label in enumerate(unique_labels):\n",
        "    train_labels_one_hot[train_labels == label, i] = 1\n",
        "\n",
        "# Séparation des pixels pour les images A et B dans les données de test\n",
        "test_pixels_a = test[:, 1:785]  # Colonnes 1 à 784 pour Image A\n",
        "test_pixels_b = test[:, 785:]  # Colonnes 785 à 1568 pour Image B\n",
        "\n",
        "# Normalisation des données d'entraînement\n",
        "train_mean = train_pixels.mean(axis=0)\n",
        "train_std = train_pixels.std(axis=0)\n",
        "train_pixels_normalized = (train_pixels - train_mean) / train_std\n",
        "\n",
        "# Division des données d'entraînement en ensembles d'entraînement et de validation\n",
        "split_ratio = int(ratio * len(train_pixels_normalized))\n",
        "split_train_pixels = train_pixels_normalized[:split_ratio]\n",
        "split_validation_pixels = train_pixels_normalized[split_ratio:]\n",
        "split_train_labels = train_labels[:split_ratio]\n",
        "split_validation_labels = train_labels[split_ratio:]\n",
        "split_train_labels_one_hot = train_labels_one_hot[:split_ratio]\n",
        "split_validation_labels_one_hot = train_labels_one_hot[split_ratio:]\n",
        "# Division des pixels de validation en images A et B\n",
        "split_validation_pixels_a = split_validation_pixels[:, :784]  # Première moitié pour l'image A\n",
        "split_validation_pixels_b = split_validation_pixels[:, 784:]  # Seconde moitié pour l'image B\n",
        "\n",
        "# Normalisation des images de test\n",
        "test_pixels_a_normalized = (test_pixels_a - train_mean) / train_std\n",
        "test_pixels_b_normalized = (test_pixels_b - train_mean) / train_std\n",
        "\n",
        "\n",
        "###################### -- Import XGBoost  -- ######################\n",
        "# Implémentation de XGBoost\n",
        "class XGBoostClassifier:\n",
        "    def __init__(self, max_depth, eta, num_class):\n",
        "        # Paramètres de l'algorithme\n",
        "        self.params = {\n",
        "            'objective': 'multi:softmax',\n",
        "            'num_class': num_class,\n",
        "            'booster': 'dart',\n",
        "            'eval_metric': 'merror',\n",
        "            'eta': eta,\n",
        "            'max_depth': max_depth,\n",
        "        }\n",
        "\n",
        "    # Entrainement du modèle\n",
        "    def train(self, train_data, train_labels, validation_data, validation_labels):\n",
        "        dtrain = xgb.DMatrix(train_data, label=train_labels)\n",
        "        dval = xgb.DMatrix(validation_data, label=validation_labels)\n",
        "        watchlist = [(dtrain, 'train'), (dval, 'validation')]\n",
        "        self.model = xgb.train(self.params, dtrain, num_boost_round=200, evals=watchlist, early_stopping_rounds=20, verbose_eval=False)\n",
        "\n",
        "    # Calcul des prédictions pour les images A et B\n",
        "    def compute_predictions(self, data_a, data_b):\n",
        "        ddata_a = xgb.DMatrix(data_a)\n",
        "        ddata_b = xgb.DMatrix(data_b)\n",
        "        preds_a = self.model.predict(ddata_a)\n",
        "        preds_b = self.model.predict(ddata_b)\n",
        "\n",
        "        # Convertir les prédictions en valeurs ASCII et les traiter selon les règles spécifiées\n",
        "        ascii_a = preds_a + 65  # ASCII pour les majuscules\n",
        "        ascii_b = preds_b + 65\n",
        "        ascii_sum = ascii_a + ascii_b\n",
        "        # Traiter la somme ASCII\n",
        "        ascii_sum_adjusted = np.where(ascii_sum > 122, ascii_sum - 57, ascii_sum)  # 57 = 122 - 65 + 1\n",
        "        final_chars = [chr(int(val)) for val in ascii_sum_adjusted]\n",
        "\n",
        "        return final_chars\n",
        "\n",
        "    # Calcul de l'accuracy (à adapter selon les besoins)\n",
        "    def compute_accuracy(self, preds, labels):\n",
        "        return np.mean(preds == labels)\n",
        "\n",
        "###################### -- Code d'entrainement XGBoost -- ######################\n",
        "def startXGBOOST():\n",
        "    learning_rates = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
        "    max_depths = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
        "    learning_rates = [0.1]\n",
        "    max_depths = [3]\n",
        "    results_XGB = []\n",
        "    best_acc_xgb = 0\n",
        "    best_lr = None\n",
        "    best_depth = None\n",
        "\n",
        "    def train(lr, depth):\n",
        "        nonlocal best_acc_xgb, best_lr, best_depth\n",
        "        print(f\"Training with learning rate: {lr}, max_depth: {depth}\\n\")\n",
        "        xgb_model = XGBoostClassifier(max_depth=depth, eta=lr, num_class=26)  # 26 classes (A-Z excluant J et Z)\n",
        "\n",
        "        # Entrainement du modèle avec les données d'entraînement\n",
        "        xgb_model.train(split_train_pixels, split_train_labels, split_validation_pixels, split_validation_labels)\n",
        "\n",
        "        # Calcul des prédictions pour les données de validation\n",
        "        val_preds_a = xgb_model.compute_predictions(split_validation_pixels_a)  # Image A\n",
        "        val_preds_b = xgb_model.compute_predictions(split_validation_pixels_b)  # Image B\n",
        "\n",
        "        # Convertir les prédictions en ASCII et les sommer\n",
        "        val_final_preds = [chr(int(a + b)) for a, b in zip(val_preds_a, val_preds_b)]\n",
        "\n",
        "        # Calculer l'accuracy (à adapter en fonction des besoins)\n",
        "        acc = xgb_model.compute_accuracy(val_final_preds, split_validation_labels)  # Nécessite un ajustement pour correspondre à la logique de la compétition\n",
        "\n",
        "        if acc > best_acc_xgb:\n",
        "            best_acc_xgb = acc\n",
        "            best_lr = lr\n",
        "            best_depth = depth\n",
        "\n",
        "        print(f\"Accuracy for learning rate: {lr}, max_depth: {depth} = {acc}\\n\")\n",
        "        results_XGB.append((lr, depth, acc))\n",
        "\n",
        "    threads = []\n",
        "    for lr in learning_rates:\n",
        "        for depth in max_depths:\n",
        "            t = threading.Thread(target=train, args=(lr, depth))\n",
        "            threads.append(t)\n",
        "            t.start()\n",
        "\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "\n",
        "    results_XGB.sort(key=lambda x: (x[0], x[1]))\n",
        "    print(f\"Best accuracy: {best_acc_xgb} with learning rate: {best_lr}, max_depth: {best_depth}\\n\")\n",
        "\n",
        "    best_xgb_model = XGBoostClassifier(max_depth=best_depth, eta=best_lr, num_class=26)\n",
        "    best_xgb_model.train(split_train_pixels, split_train_labels, split_validation_pixels, split_validation_labels)\n",
        "\n",
        "    test_preds_a = best_xgb_model.compute_predictions(test_pixels_a_normalized)\n",
        "    test_preds_b = best_xgb_model.compute_predictions(test_pixels_b_normalized)\n",
        "\n",
        "    test_final_preds = [chr(int(a + b)) for a, b in zip(test_preds_a, test_preds_b)]\n",
        "\n",
        "    save_predictions_to_csv(f\"xgb_predictions_lr{best_lr}_depth{best_depth}.csv\", test_final_preds)\n",
        "\n",
        "    plot_results_XGB(results_XGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "startXGBOOST()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPmu64ganIY6UaM8ELdy5fT",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
