{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import random\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Setting a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "###################### -- HELPER FUNCTIONS -- ######################\n",
        "def merge_predictions(preds_a, preds_b):\n",
        "    merged_predictions = []\n",
        "    for i in range(len(preds_a)):\n",
        "        \n",
        "        # Replacing 9 back to 24 if needed\n",
        "        decoded_a = preds_a[i] if preds_a[i] != 9 else 24\n",
        "        decoded_b = preds_b[i] if preds_b[i] != 9 else 24\n",
        "\n",
        "        ascii_a = decoded_a + 65\n",
        "        ascii_b = decoded_b + 65\n",
        "\n",
        "        sum_pred = normalize_ascii_sum(ascii_a + ascii_b)\n",
        "        merged_predictions.append((i, chr(sum_pred)))\n",
        "    return merged_predictions\n",
        "\n",
        "def separate_test_sets(file_path):\n",
        "    test_data = pd.read_csv(file_path)\n",
        "    test_a_columns = [col for col in test_data.columns if 'pixel_a' in col]\n",
        "    test_b_columns = [col for col in test_data.columns if 'pixel_b' in col]\n",
        "    test_a = test_data[test_a_columns]\n",
        "    test_b = test_data[test_b_columns]\n",
        "    test_a.columns = [col.replace('_a', '') for col in test_a.columns]\n",
        "    test_b.columns = [col.replace('_b', '') for col in test_b.columns]\n",
        "    return test_a, test_b\n",
        "\n",
        "def normalize_ascii_sum(ascii_sum):\n",
        "    while ascii_sum > 122:  # 'z' is ASCII 122\n",
        "        ascii_sum -= 65  # 122 ('z') - 65 ('A') + 1\n",
        "    return int(ascii_sum)\n",
        "\n",
        "def save_predictions_to_csv(filename, predictions):\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(\"id,label\\n\")\n",
        "        for id, label in predictions:\n",
        "            file.write(f\"{id},{label}\\n\")\n",
        "\n",
        "###################### -- Random Forest Custom  -- ######################\n",
        "\n",
        "class RandomForestClassifier:\n",
        "    def __init__(self, n_estimators=100, max_features=3, max_depth=10, min_samples_split=2):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = max_features\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.trees = []\n",
        "\n",
        "    @staticmethod\n",
        "    def _information_gain(left_child, right_child):\n",
        "        parent = left_child + right_child\n",
        "        p_parent = parent.count(1) / len(parent) if len(parent) > 0 else 0\n",
        "        p_left = left_child.count(1) / len(left_child) if len(left_child) > 0 else 0\n",
        "        p_right = right_child.count(1) / len(right_child) if len(right_child) > 0 else 0\n",
        "        IG_p = entropy(p_parent)\n",
        "        IG_l = entropy(p_left)\n",
        "        IG_r = entropy(p_right)\n",
        "        return IG_p - len(left_child) / len(parent) * IG_l - len(right_child) / len(parent) * IG_r\n",
        "\n",
        "    @staticmethod\n",
        "    def _draw_bootstrap(X_train, y_train):\n",
        "        bootstrap_indices = list(np.random.choice(range(len(X_train)), len(X_train), replace=True))\n",
        "        oob_indices = [i for i in range(len(X_train)) if i not in bootstrap_indices]\n",
        "        X_bootstrap = X_train.iloc[bootstrap_indices].values\n",
        "        y_bootstrap = y_train[bootstrap_indices]\n",
        "        X_oob = X_train.iloc[oob_indices].values\n",
        "        y_oob = y_train[oob_indices]\n",
        "        return X_bootstrap, y_bootstrap, X_oob, y_oob\n",
        "\n",
        "    @staticmethod\n",
        "    def _oob_score(tree, X_test, y_test):\n",
        "        mis_label = 0\n",
        "        for i in range(len(X_test)):\n",
        "            pred = RandomForestClassifier._predict_tree(tree, X_test[i])\n",
        "            if pred != y_test[i]:\n",
        "                mis_label += 1\n",
        "        return mis_label / len(X_test)\n",
        "\n",
        "    @staticmethod\n",
        "    def _find_split_point(X_bootstrap, y_bootstrap, max_features):\n",
        "        feature_ls = list()\n",
        "        num_features = len(X_bootstrap[0])\n",
        "\n",
        "        while len(feature_ls) <= max_features:\n",
        "            feature_idx = random.sample(range(num_features), 1)\n",
        "            if feature_idx not in feature_ls:\n",
        "                feature_ls.extend(feature_idx)\n",
        "\n",
        "        best_info_gain = -999\n",
        "        node = None\n",
        "        for feature_idx in feature_ls:\n",
        "            for split_point in X_bootstrap[:, feature_idx]:\n",
        "                left_child = {'X_bootstrap': [], 'y_bootstrap': []}\n",
        "                right_child = {'X_bootstrap': [], 'y_bootstrap': []}\n",
        "\n",
        "                if type(split_point) in [int, float]:\n",
        "                    for i, value in enumerate(X_bootstrap[:, feature_idx]):\n",
        "                        if value <= split_point:\n",
        "                            left_child['X_bootstrap'].append(X_bootstrap[i])\n",
        "                            left_child['y_bootstrap'].append(y_bootstrap[i])\n",
        "                        else:\n",
        "                            right_child['X_bootstrap'].append(X_bootstrap[i])\n",
        "                            right_child['y_bootstrap'].append(y_bootstrap[i])\n",
        "                else:\n",
        "                    for i, value in enumerate(X_bootstrap[:, feature_idx]):\n",
        "                        if value == split_point:\n",
        "                            left_child['X_bootstrap'].append(X_bootstrap[i])\n",
        "                            left_child['y_bootstrap'].append(y_bootstrap[i])\n",
        "                        else:\n",
        "                            right_child['X_bootstrap'].append(X_bootstrap[i])\n",
        "                            right_child['y_bootstrap'].append(y_bootstrap[i])\n",
        "\n",
        "                split_info_gain = RandomForestClassifier._information_gain(left_child['y_bootstrap'], right_child['y_bootstrap'])\n",
        "                if split_info_gain > best_info_gain:\n",
        "                    best_info_gain = split_info_gain\n",
        "                    left_child['X_bootstrap'] = np.array(left_child['X_bootstrap'])\n",
        "                    right_child['X_bootstrap'] = np.array(right_child['X_bootstrap'])\n",
        "                    node = {'information_gain': split_info_gain,\n",
        "                            'left_child': left_child,\n",
        "                            'right_child': right_child,\n",
        "                            'split_point': split_point,\n",
        "                            'feature_idx': feature_idx}\n",
        "        return node\n",
        "\n",
        "    @staticmethod\n",
        "    def _terminal_node(node):\n",
        "        y_bootstrap = node['y_bootstrap']\n",
        "        return max(y_bootstrap, key=y_bootstrap.count)\n",
        "\n",
        "    @staticmethod\n",
        "    def _split_node(node, max_features, min_samples_split, max_depth, depth):\n",
        "        left_child = node['left_child']\n",
        "        right_child = node['right_child']\n",
        "\n",
        "        if len(left_child['y_bootstrap']) == 0 or len(right_child['y_bootstrap']) == 0:\n",
        "            empty_child = {'y_bootstrap': left_child['y_bootstrap'] + right_child['y_bootstrap']}\n",
        "            node['left_split'] = RandomForestClassifier._terminal_node(empty_child)\n",
        "            node['right_split'] = RandomForestClassifier._terminal_node(empty_child)\n",
        "            return\n",
        "\n",
        "        if depth >= max_depth:\n",
        "            node['left_split'] = RandomForestClassifier._terminal_node(left_child)\n",
        "            node['right_split'] = RandomForestClassifier._terminal_node(right_child)\n",
        "            return\n",
        "\n",
        "        if len(left_child['X_bootstrap']) <= min_samples_split:\n",
        "            node['left_split'] = RandomForestClassifier._terminal_node(left_child)\n",
        "        else:\n",
        "            node['left_split'] = RandomForestClassifier._find_split_point(left_child['X_bootstrap'], left_child['y_bootstrap'], max_features)\n",
        "            RandomForestClassifier._split_node(node['left_split'], max_features, min_samples_split, max_depth, depth + 1)\n",
        "\n",
        "        if len(right_child['X_bootstrap']) <= min_samples_split:\n",
        "            node['right_split'] = RandomForestClassifier._terminal_node(right_child)\n",
        "        else:\n",
        "            node['right_split'] = RandomForestClassifier._find_split_point(right_child['X_bootstrap'], right_child['y_bootstrap'], max_features)\n",
        "            RandomForestClassifier._split_node(node['right_split'], max_features, min_samples_split, max_depth, depth + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_tree(X_bootstrap, y_bootstrap, max_depth, min_samples_split, max_features):\n",
        "        root_node = RandomForestClassifier._find_split_point(X_bootstrap, y_bootstrap, max_features)\n",
        "        RandomForestClassifier._split_node(root_node, max_features, min_samples_split, max_depth, 1)\n",
        "        return root_node\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        for _ in range(self.n_estimators):\n",
        "            X_bootstrap, y_bootstrap, _, _ = RandomForestClassifier._draw_bootstrap(X_train, y_train)\n",
        "            tree = RandomForestClassifier._build_tree(X_bootstrap, y_bootstrap, self.max_depth, self.min_samples_split, self.max_features)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    @staticmethod\n",
        "    def _predict_tree(tree, x):\n",
        "        feature_idx = tree['feature_idx']\n",
        "        if x[feature_idx] <= tree['split_point']:\n",
        "            if type(tree['left_split']) == dict:\n",
        "                return RandomForestClassifier._predict_tree(tree['left_split'], x)\n",
        "            else:\n",
        "                return tree['left_split']\n",
        "        else:\n",
        "            if type(tree['right_split']) == dict:\n",
        "                return RandomForestClassifier._predict_tree(tree['right_split'], x)\n",
        "            else:\n",
        "                return tree['right_split']\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        preds = [max(set(RandomForestClassifier._predict_tree(tree, x) for tree in self.trees), key=list.count) for x in X_test]\n",
        "        return preds\n",
        "\n",
        "def startRandomForest(X_train, y_train, X_val, y_val):\n",
        "    # Définir les grilles de recherche pour les hyperparamètres\n",
        "    n_estimators_options = [100, 200, 300]\n",
        "    max_features_options = [3, 5, 7]\n",
        "    max_depth_options = [10, 15, 20]\n",
        "    min_samples_split_options = [2, 4, 6]\n",
        "\n",
        "    best_forest = None\n",
        "    best_accuracy = 0\n",
        "    best_params = None\n",
        "\n",
        "    for n_estimators in n_estimators_options:\n",
        "        for max_features in max_features_options:\n",
        "            for max_depth in max_depth_options:\n",
        "                for min_samples_split in min_samples_split_options:\n",
        "                    forest = RandomForestClassifier(n_estimators, max_features, max_depth, min_samples_split)\n",
        "                    forest.fit(X_train, y_train)\n",
        "                    # Évaluer sur l'ensemble de validation\n",
        "                    predictions = forest.predict(X_val)\n",
        "                    acc = sum(predictions == y_val) / len(y_val)\n",
        "                    print(f\"n_estimators: {n_estimators} ; max_features: {max_features} ; Depth: {max_depth} ; min_samples_split: {min_samples_split} ; Accuracy: {acc * 100:.2f}%\")\n",
        "                    if acc > best_accuracy:\n",
        "                        best_accuracy = acc\n",
        "                        best_forest = forest\n",
        "                        best_params = (n_estimators, max_features, max_depth, min_samples_split)\n",
        "                        print(f\"New best: {best_params} // Accuracy : {best_accuracy}\")\n",
        "\n",
        "    print(f\"Best model : {best_params} // Accuracy : {best_accuracy}\")\n",
        "    return best_forest\n",
        "\n",
        "\n",
        "###################### -- CNN IMPLEMENTATION -- ######################\n",
        "\n",
        "class CNNClassifier:\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.model = self._build_model(input_shape, num_classes)\n",
        "\n",
        "    def _build_model(self, input_shape, num_classes):\n",
        "        model = Sequential([\n",
        "            Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
        "            BatchNormalization(),\n",
        "            Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D((2, 2)),\n",
        "            Dropout(0.25),\n",
        "            Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D((2, 2)),\n",
        "            Dropout(0.25),\n",
        "            Flatten(),\n",
        "            Dense(512, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.5),\n",
        "            Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
        "        model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "    def train(self, x_train, y_train, x_val, y_val, epochs=50, batch_size=64):\n",
        "        datagen = ImageDataGenerator(\n",
        "            rotation_range=10,\n",
        "            zoom_range=0.1,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1\n",
        "        )\n",
        "        datagen.fit(x_train)\n",
        "        reduce_lr = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n",
        "        self.model.fit(\n",
        "            datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_val, y_val),\n",
        "            callbacks=[reduce_lr]\n",
        "        )\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        return self.model.predict(x_test)\n",
        "\n",
        "def startCNN(X, Y, input_shape, num_classes):\n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
        "    cnn = CNNClassifier(input_shape, num_classes)\n",
        "    cnn.train(X_train, Y_train, X_val, Y_val)\n",
        "    return cnn\n",
        "\n",
        "###################### -- XGBOOST Implementation -- ######################\n",
        "\n",
        "class XGBoostClassifier:\n",
        "    def __init__(self, max_depth=7, eta=0.1, num_class=24):\n",
        "        self.params = {\n",
        "            'objective': 'multi:softmax',\n",
        "            'num_class': num_class,\n",
        "            'booster': 'gbtree',\n",
        "            'eval_metric': 'merror',\n",
        "            'eta': eta,\n",
        "            'max_depth': max_depth,\n",
        "        }\n",
        "\n",
        "    def train(self, train_data, train_labels, validation_data, validation_labels):\n",
        "        dtrain = xgb.DMatrix(train_data, label=train_labels)\n",
        "        dval = xgb.DMatrix(validation_data, label=validation_labels)\n",
        "        watchlist = [(dtrain, 'train'), (dval, 'validation')]\n",
        "        self.model = xgb.train(self.params, dtrain, num_boost_round=200, evals=watchlist, early_stopping_rounds=20, verbose_eval=False)\n",
        "\n",
        "    def compute_predictions(self, data):\n",
        "        ddata = xgb.DMatrix(data)\n",
        "        return self.model.predict(ddata)\n",
        "\n",
        "    def compute_accuracy(self, preds, labels):\n",
        "        return np.mean(preds == labels)\n",
        "\n",
        "def startXGBOOST(train_data_normalized, train_labels, validation_data_normalized, validation_labels):\n",
        "    learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "    max_depths = [3, 4, 5, 6, 7, 8, 9, 10]\n",
        "    learning_rates = [0.1]\n",
        "    max_depths = [7]\n",
        "    results_XGB = []\n",
        "    best_acc_xgb = 0\n",
        "    best_lr = None\n",
        "    best_depth = None\n",
        "    num_class = len(np.unique(train_labels))\n",
        "    print(num_class)\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        for depth in max_depths:\n",
        "            xgb_model = XGBoostClassifier(max_depth=depth, eta=lr, num_class=num_class)\n",
        "            xgb_model.train(train_data_normalized, train_labels, validation_data_normalized, validation_labels)\n",
        "            val_preds = xgb_model.compute_predictions(validation_data_normalized)\n",
        "            acc = xgb_model.compute_accuracy(val_preds, validation_labels)\n",
        "            print(f\"LR: {lr} ; Depth: {depth} ; Accuracy: {acc * 100:.2f}%\")\n",
        "            if acc > best_acc_xgb:\n",
        "                best_acc_xgb = acc\n",
        "                best_lr = lr\n",
        "                best_depth = depth\n",
        "                print(\"New best\")\n",
        "            results_XGB.append((lr, depth, acc))\n",
        "\n",
        "    best_xgb_model = XGBoostClassifier(max_depth=best_depth, eta=best_lr, num_class=num_class)\n",
        "    best_xgb_model.train(train_data_normalized, train_labels, validation_data_normalized, validation_labels)\n",
        "\n",
        "    return best_xgb_model\n",
        "\n",
        "###################### -- DATA HANDLING AND TRAINING -- ######################\n",
        "\n",
        "# Load your dataset here\n",
        "mnist_sign_train = pd.read_csv('sign_mnist_train.csv')\n",
        "test_a, test_b = separate_test_sets('test.csv')\n",
        "\n",
        "mnist_sign_train['label'] = mnist_sign_train['label'].replace(24, 9) ## Because\n",
        "\n",
        "# Préparation des données de training pour XGBoost & CNN\n",
        "features_xgb = mnist_sign_train.drop('label', axis=1).values / 255.0\n",
        "labels_xgb = mnist_sign_train['label'].values\n",
        "features_cnn = features_xgb.reshape((-1, 28, 28, 1))\n",
        "labels_cnn = tf.keras.utils.to_categorical(labels_xgb, num_classes=25)\n",
        "\n",
        "# Préparation des données de test pour XGBoost & CNN\n",
        "normalized_test_a_cnn = test_a.values.reshape((-1, 28, 28, 1)) / 255.0\n",
        "normalized_test_b_cnn = test_b.values.reshape((-1, 28, 28, 1)) / 255.0\n",
        "normalized_test_a_xgb = test_a.values / 255.0\n",
        "normalized_test_b_xgb = test_b.values / 255.0\n",
        "\n",
        "#Split Training Validation pour XGB\n",
        "split_index = int(0.8 * len(features_xgb))\n",
        "x_train_xgb, x_val_xgb = features_xgb[:split_index], features_xgb[split_index:]\n",
        "y_train_xgb, y_val_xgb = labels_xgb[:split_index], labels_xgb[split_index:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR: 0.01 ; Depth: 3 ; Accuracy: 77.13%\n",
            "New best\n",
            "LR: 0.01 ; Depth: 4 ; Accuracy: 87.91%\n",
            "New best\n",
            "LR: 0.01 ; Depth: 5 ; Accuracy: 92.73%\n",
            "New best\n",
            "LR: 0.01 ; Depth: 6 ; Accuracy: 95.16%\n",
            "New best\n",
            "LR: 0.01 ; Depth: 7 ; Accuracy: 96.30%\n",
            "New best\n",
            "LR: 0.01 ; Depth: 8 ; Accuracy: 96.78%\n",
            "New best\n",
            "LR: 0.01 ; Depth: 9 ; Accuracy: 97.32%\n",
            "New best\n",
            "LR: 0.01 ; Depth: 10 ; Accuracy: 97.47%\n",
            "New best\n",
            "LR: 0.05 ; Depth: 3 ; Accuracy: 95.81%\n",
            "LR: 0.05 ; Depth: 4 ; Accuracy: 98.22%\n",
            "New best\n",
            "LR: 0.05 ; Depth: 5 ; Accuracy: 98.83%\n",
            "New best\n",
            "LR: 0.05 ; Depth: 6 ; Accuracy: 99.11%\n",
            "New best\n",
            "LR: 0.05 ; Depth: 7 ; Accuracy: 99.27%\n",
            "New best\n",
            "LR: 0.05 ; Depth: 8 ; Accuracy: 99.18%\n",
            "LR: 0.05 ; Depth: 9 ; Accuracy: 99.25%\n",
            "LR: 0.05 ; Depth: 10 ; Accuracy: 99.29%\n",
            "New best\n",
            "LR: 0.1 ; Depth: 3 ; Accuracy: 98.63%\n",
            "LR: 0.1 ; Depth: 4 ; Accuracy: 99.27%\n",
            "LR: 0.1 ; Depth: 5 ; Accuracy: 99.38%\n",
            "New best\n",
            "LR: 0.1 ; Depth: 6 ; Accuracy: 99.42%\n",
            "New best\n",
            "LR: 0.1 ; Depth: 7 ; Accuracy: 99.54%\n",
            "New best\n",
            "LR: 0.1 ; Depth: 8 ; Accuracy: 99.53%\n",
            "LR: 0.1 ; Depth: 9 ; Accuracy: 99.38%\n",
            "LR: 0.1 ; Depth: 10 ; Accuracy: 99.24%\n",
            "LR: 0.2 ; Depth: 3 ; Accuracy: 99.44%\n",
            "LR: 0.2 ; Depth: 4 ; Accuracy: 99.29%\n",
            "LR: 0.2 ; Depth: 5 ; Accuracy: 99.40%\n",
            "LR: 0.2 ; Depth: 6 ; Accuracy: 99.31%\n",
            "LR: 0.2 ; Depth: 7 ; Accuracy: 99.53%\n",
            "LR: 0.2 ; Depth: 8 ; Accuracy: 99.45%\n",
            "LR: 0.2 ; Depth: 9 ; Accuracy: 99.25%\n",
            "LR: 0.2 ; Depth: 10 ; Accuracy: 99.51%\n",
            "LR: 0.3 ; Depth: 3 ; Accuracy: 99.44%\n",
            "LR: 0.3 ; Depth: 4 ; Accuracy: 99.38%\n",
            "LR: 0.3 ; Depth: 5 ; Accuracy: 99.38%\n",
            "LR: 0.3 ; Depth: 6 ; Accuracy: 99.25%\n",
            "LR: 0.3 ; Depth: 7 ; Accuracy: 99.47%\n",
            "LR: 0.3 ; Depth: 8 ; Accuracy: 99.34%\n",
            "LR: 0.3 ; Depth: 9 ; Accuracy: 99.47%\n",
            "LR: 0.3 ; Depth: 10 ; Accuracy: 99.38%\n"
          ]
        }
      ],
      "source": [
        "# Train XGB Model\n",
        "best_xgb_model = startXGBOOST(x_train_xgb, y_train_xgb, x_val_xgb, y_val_xgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "387/387 [==============================] - 39s 97ms/step - loss: 1.0202 - accuracy: 0.6962 - val_loss: 3.4025 - val_accuracy: 0.2145 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "387/387 [==============================] - 36s 93ms/step - loss: 0.2316 - accuracy: 0.9254 - val_loss: 0.0188 - val_accuracy: 0.9975 - lr: 9.0000e-04\n",
            "Epoch 3/50\n",
            "387/387 [==============================] - 37s 94ms/step - loss: 0.1165 - accuracy: 0.9636 - val_loss: 0.2669 - val_accuracy: 0.8922 - lr: 8.1000e-04\n",
            "Epoch 4/50\n",
            "387/387 [==============================] - 36s 93ms/step - loss: 0.0828 - accuracy: 0.9739 - val_loss: 0.0025 - val_accuracy: 1.0000 - lr: 7.2900e-04\n",
            "Epoch 5/50\n",
            "387/387 [==============================] - 40s 103ms/step - loss: 0.0630 - accuracy: 0.9813 - val_loss: 0.0035 - val_accuracy: 1.0000 - lr: 6.5610e-04\n",
            "Epoch 6/50\n",
            "387/387 [==============================] - 38s 98ms/step - loss: 0.0501 - accuracy: 0.9853 - val_loss: 0.0013 - val_accuracy: 1.0000 - lr: 5.9049e-04\n",
            "Epoch 7/50\n",
            "387/387 [==============================] - 35s 89ms/step - loss: 0.0337 - accuracy: 0.9898 - val_loss: 4.0687e-04 - val_accuracy: 1.0000 - lr: 5.3144e-04\n",
            "Epoch 8/50\n",
            "387/387 [==============================] - 34s 88ms/step - loss: 0.0258 - accuracy: 0.9926 - val_loss: 1.2123e-04 - val_accuracy: 1.0000 - lr: 4.7830e-04\n",
            "Epoch 9/50\n",
            "387/387 [==============================] - 37s 96ms/step - loss: 0.0201 - accuracy: 0.9935 - val_loss: 1.2893e-04 - val_accuracy: 1.0000 - lr: 4.3047e-04\n",
            "Epoch 10/50\n",
            "387/387 [==============================] - 34s 89ms/step - loss: 0.0186 - accuracy: 0.9944 - val_loss: 0.0422 - val_accuracy: 0.9847 - lr: 3.8742e-04\n",
            "Epoch 11/50\n",
            "387/387 [==============================] - 34s 87ms/step - loss: 0.0228 - accuracy: 0.9936 - val_loss: 3.4788e-05 - val_accuracy: 1.0000 - lr: 3.4868e-04\n",
            "Epoch 12/50\n",
            "387/387 [==============================] - 33s 86ms/step - loss: 0.0145 - accuracy: 0.9960 - val_loss: 6.5752e-05 - val_accuracy: 1.0000 - lr: 3.1381e-04\n",
            "Epoch 13/50\n",
            "387/387 [==============================] - 34s 87ms/step - loss: 0.0109 - accuracy: 0.9970 - val_loss: 1.2230e-04 - val_accuracy: 1.0000 - lr: 2.8243e-04\n",
            "Epoch 14/50\n",
            "387/387 [==============================] - 36s 93ms/step - loss: 0.0100 - accuracy: 0.9971 - val_loss: 1.6613e-04 - val_accuracy: 1.0000 - lr: 2.5419e-04\n",
            "Epoch 15/50\n",
            "387/387 [==============================] - 35s 90ms/step - loss: 0.0134 - accuracy: 0.9962 - val_loss: 5.9261e-05 - val_accuracy: 1.0000 - lr: 2.2877e-04\n",
            "Epoch 16/50\n",
            "387/387 [==============================] - 35s 89ms/step - loss: 0.0110 - accuracy: 0.9970 - val_loss: 5.0020e-05 - val_accuracy: 1.0000 - lr: 2.0589e-04\n",
            "Epoch 17/50\n",
            "387/387 [==============================] - 35s 90ms/step - loss: 0.0099 - accuracy: 0.9973 - val_loss: 8.5617e-06 - val_accuracy: 1.0000 - lr: 1.8530e-04\n",
            "Epoch 18/50\n",
            "387/387 [==============================] - 35s 91ms/step - loss: 0.0081 - accuracy: 0.9978 - val_loss: 5.4791e-06 - val_accuracy: 1.0000 - lr: 1.6677e-04\n",
            "Epoch 19/50\n",
            "387/387 [==============================] - 35s 89ms/step - loss: 0.0078 - accuracy: 0.9981 - val_loss: 9.4565e-06 - val_accuracy: 1.0000 - lr: 1.5009e-04\n",
            "Epoch 20/50\n",
            "387/387 [==============================] - 34s 89ms/step - loss: 0.0062 - accuracy: 0.9984 - val_loss: 3.6518e-05 - val_accuracy: 1.0000 - lr: 1.3509e-04\n",
            "Epoch 21/50\n",
            "387/387 [==============================] - 33s 86ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 5.5505e-06 - val_accuracy: 1.0000 - lr: 1.2158e-04\n",
            "Epoch 22/50\n",
            "387/387 [==============================] - 34s 87ms/step - loss: 0.0055 - accuracy: 0.9980 - val_loss: 3.9045e-06 - val_accuracy: 1.0000 - lr: 1.0942e-04\n",
            "Epoch 23/50\n",
            "387/387 [==============================] - 33s 86ms/step - loss: 0.0065 - accuracy: 0.9980 - val_loss: 4.1911e-06 - val_accuracy: 1.0000 - lr: 9.8477e-05\n",
            "Epoch 24/50\n",
            "387/387 [==============================] - 33s 85ms/step - loss: 0.0066 - accuracy: 0.9980 - val_loss: 4.0141e-06 - val_accuracy: 1.0000 - lr: 8.8629e-05\n",
            "Epoch 25/50\n",
            "387/387 [==============================] - 37s 96ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 2.2736e-06 - val_accuracy: 1.0000 - lr: 7.9766e-05\n",
            "Epoch 26/50\n",
            "387/387 [==============================] - 36s 92ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 2.2375e-06 - val_accuracy: 1.0000 - lr: 7.1790e-05\n",
            "Epoch 27/50\n",
            "387/387 [==============================] - 35s 90ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 1.7661e-06 - val_accuracy: 1.0000 - lr: 6.4611e-05\n",
            "Epoch 28/50\n",
            "387/387 [==============================] - 34s 87ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 1.6396e-06 - val_accuracy: 1.0000 - lr: 5.8150e-05\n",
            "Epoch 29/50\n",
            "387/387 [==============================] - 34s 88ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 1.5723e-06 - val_accuracy: 1.0000 - lr: 5.2335e-05\n",
            "Epoch 30/50\n",
            "387/387 [==============================] - 34s 89ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 1.6602e-06 - val_accuracy: 1.0000 - lr: 4.7101e-05\n",
            "Epoch 31/50\n",
            "387/387 [==============================] - 35s 91ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 1.4018e-06 - val_accuracy: 1.0000 - lr: 4.2391e-05\n",
            "Epoch 32/50\n",
            "387/387 [==============================] - 36s 92ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 1.9054e-06 - val_accuracy: 1.0000 - lr: 3.8152e-05\n",
            "Epoch 33/50\n",
            "387/387 [==============================] - 36s 92ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 9.6443e-07 - val_accuracy: 1.0000 - lr: 3.4337e-05\n",
            "Epoch 34/50\n",
            "387/387 [==============================] - 35s 89ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 7.9843e-07 - val_accuracy: 1.0000 - lr: 3.0903e-05\n",
            "Epoch 35/50\n",
            "387/387 [==============================] - 34s 89ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 7.4642e-07 - val_accuracy: 1.0000 - lr: 2.7813e-05\n",
            "Epoch 36/50\n",
            "387/387 [==============================] - 36s 92ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 9.0960e-07 - val_accuracy: 1.0000 - lr: 2.5032e-05\n",
            "Epoch 37/50\n",
            "387/387 [==============================] - 36s 93ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 7.6300e-07 - val_accuracy: 1.0000 - lr: 2.2528e-05\n",
            "Epoch 38/50\n",
            "387/387 [==============================] - 35s 91ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 7.5501e-07 - val_accuracy: 1.0000 - lr: 2.0276e-05\n",
            "Epoch 39/50\n",
            "387/387 [==============================] - 35s 91ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 9.5601e-07 - val_accuracy: 1.0000 - lr: 1.8248e-05\n",
            "Epoch 40/50\n",
            "387/387 [==============================] - 36s 93ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 7.0366e-07 - val_accuracy: 1.0000 - lr: 1.6423e-05\n",
            "Epoch 41/50\n",
            "387/387 [==============================] - 35s 90ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 6.5226e-07 - val_accuracy: 1.0000 - lr: 1.4781e-05\n",
            "Epoch 42/50\n",
            "387/387 [==============================] - 35s 90ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 7.5706e-07 - val_accuracy: 1.0000 - lr: 1.3303e-05\n",
            "Epoch 43/50\n",
            "387/387 [==============================] - 35s 89ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 6.2704e-07 - val_accuracy: 1.0000 - lr: 1.1973e-05\n",
            "Epoch 44/50\n",
            "387/387 [==============================] - 34s 89ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 7.4607e-07 - val_accuracy: 1.0000 - lr: 1.0775e-05\n",
            "Epoch 45/50\n",
            "387/387 [==============================] - 35s 90ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 6.3833e-07 - val_accuracy: 1.0000 - lr: 9.6977e-06\n",
            "Epoch 46/50\n",
            "387/387 [==============================] - 35s 90ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 6.3720e-07 - val_accuracy: 1.0000 - lr: 8.7280e-06\n",
            "Epoch 47/50\n",
            "387/387 [==============================] - 34s 87ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 6.6997e-07 - val_accuracy: 1.0000 - lr: 7.8552e-06\n",
            "Epoch 48/50\n",
            "387/387 [==============================] - 35s 90ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 6.5139e-07 - val_accuracy: 1.0000 - lr: 7.0697e-06\n",
            "Epoch 49/50\n",
            "387/387 [==============================] - 34s 88ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 7.2029e-07 - val_accuracy: 1.0000 - lr: 6.3627e-06\n",
            "Epoch 50/50\n",
            "387/387 [==============================] - 33s 86ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 6.4024e-07 - val_accuracy: 1.0000 - lr: 5.7264e-06\n"
          ]
        }
      ],
      "source": [
        "# Train CNN Model\n",
        "cnn_model = startCNN(features_cnn, labels_cnn, input_shape=(28, 28, 1), num_classes=25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Simo\\eclipse-workspace\\Kaggle_MNIST_Sign\\Kaggle_Competition_2.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train Random Forest Model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m best_rf_model \u001b[39m=\u001b[39m startRandomForest(x_train_xgb, y_train_xgb, x_val_xgb, y_val_xgb)\n",
            "\u001b[1;32mc:\\Users\\Simo\\eclipse-workspace\\Kaggle_MNIST_Sign\\Kaggle_Competition_2.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=216'>217</a>\u001b[0m \u001b[39mfor\u001b[39;00m min_samples_split \u001b[39min\u001b[39;00m min_samples_split_options:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=217'>218</a>\u001b[0m     forest \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators, max_features, max_depth, min_samples_split)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=218'>219</a>\u001b[0m     forest\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=219'>220</a>\u001b[0m     \u001b[39m# Évaluer sur l'ensemble de validation\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=220'>221</a>\u001b[0m     predictions \u001b[39m=\u001b[39m forest\u001b[39m.\u001b[39mpredict(X_val)\n",
            "\u001b[1;32mc:\\Users\\Simo\\eclipse-workspace\\Kaggle_MNIST_Sign\\Kaggle_Competition_2.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=178'>179</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X_train, y_train):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=179'>180</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=180'>181</a>\u001b[0m         X_bootstrap, y_bootstrap, _, _ \u001b[39m=\u001b[39m RandomForestClassifier\u001b[39m.\u001b[39;49m_draw_bootstrap(X_train, y_train)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=181'>182</a>\u001b[0m         tree \u001b[39m=\u001b[39m RandomForestClassifier\u001b[39m.\u001b[39m_build_tree(X_bootstrap, y_bootstrap, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_depth, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_samples_split, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_features)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrees\u001b[39m.\u001b[39mappend(tree)\n",
            "\u001b[1;32mc:\\Users\\Simo\\eclipse-workspace\\Kaggle_MNIST_Sign\\Kaggle_Competition_2.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m bootstrap_indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X_train)), \u001b[39mlen\u001b[39m(X_train), replace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m oob_indices \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X_train)) \u001b[39mif\u001b[39;00m i \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m bootstrap_indices]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m X_bootstrap \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39;49miloc[bootstrap_indices]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m y_bootstrap \u001b[39m=\u001b[39m y_train[bootstrap_indices]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Simo/eclipse-workspace/Kaggle_MNIST_Sign/Kaggle_Competition_2.ipynb#X12sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m X_oob \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39miloc[oob_indices]\u001b[39m.\u001b[39mvalues\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
          ]
        }
      ],
      "source": [
        "# Train Random Forest Model\n",
        "best_rf_model = startRandomForest(x_train_xgb, y_train_xgb, x_val_xgb, y_val_xgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost Predictions\n",
        "xgb_preds_a = best_xgb_model.compute_predictions(normalized_test_a_xgb)\n",
        "xgb_preds_b = best_xgb_model.compute_predictions(normalized_test_b_xgb)\n",
        "xgb_merged_predictions = merge_predictions(xgb_preds_a, xgb_preds_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "94/94 [==============================] - 1s 8ms/step\n",
            "94/94 [==============================] - 1s 8ms/step\n"
          ]
        }
      ],
      "source": [
        "# CNN Predictions\n",
        "cnn_preds_a = cnn_model.predict(normalized_test_a_cnn)\n",
        "cnn_preds_b = cnn_model.predict(normalized_test_b_cnn)\n",
        "cnn_merged_predictions = merge_predictions(np.argmax(cnn_preds_a, axis=1), np.argmax(cnn_preds_b, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest Predictions\n",
        "rfc_preds_a = best_rfc_model.predict(normalized_test_a_xgb)\n",
        "rfc_preds_b = best_rfc_model.predict(normalized_test_b_xgb)\n",
        "rfc_merged_predictions = merge_predictions(rfc_preds_a, rfc_preds_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save XGB\n",
        "save_predictions_to_csv(\"xgb_predictions.csv\", xgb_merged_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save CNN\n",
        "save_predictions_to_csv(\"cnn_predictions.csv\", cnn_merged_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Random Forest\n",
        "save_predictions_to_csv(\"rfc_predictions.csv\", rfc_merged_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
