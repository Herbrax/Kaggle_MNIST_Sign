{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import random\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Setting a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "###################### -- HELPER FUNCTIONS -- ######################\n",
        "def merge_predictions(preds_a, preds_b):\n",
        "    merged_predictions = []\n",
        "    for i in range(len(preds_a)):\n",
        "        \n",
        "        # Replacing 9 back to 24 if needed\n",
        "        decoded_a = preds_a[i] if preds_a[i] != 9 else 24\n",
        "        decoded_b = preds_b[i] if preds_b[i] != 9 else 24\n",
        "\n",
        "        ascii_a = decoded_a + 65\n",
        "        ascii_b = decoded_b + 65\n",
        "\n",
        "        sum_pred = normalize_ascii_sum(ascii_a + ascii_b)\n",
        "        merged_predictions.append((i, chr(sum_pred)))\n",
        "    return merged_predictions\n",
        "\n",
        "def separate_test_sets(file_path):\n",
        "    test_data = pd.read_csv(file_path)\n",
        "    test_a_columns = [col for col in test_data.columns if 'pixel_a' in col]\n",
        "    test_b_columns = [col for col in test_data.columns if 'pixel_b' in col]\n",
        "    test_a = test_data[test_a_columns]\n",
        "    test_b = test_data[test_b_columns]\n",
        "    test_a.columns = [col.replace('_a', '') for col in test_a.columns]\n",
        "    test_b.columns = [col.replace('_b', '') for col in test_b.columns]\n",
        "    return test_a, test_b\n",
        "\n",
        "def normalize_ascii_sum(ascii_sum):\n",
        "    while ascii_sum > 122:  # 'z' is ASCII 122\n",
        "        ascii_sum -= 65  # 122 ('z') - 65 ('A') + 1\n",
        "    return int(ascii_sum)\n",
        "\n",
        "def save_predictions_to_csv(filename, predictions):\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(\"id,label\\n\")\n",
        "        for id, label in predictions:\n",
        "            file.write(f\"{id},{label}\\n\")\n",
        "\n",
        "def entropy(p):\n",
        "    return - (p * np.log2(p) + (1 - p) * np.log2(1 - p)) if 0 < p < 1 else 0\n",
        " \n",
        "###################### -- CNN IMPLEMENTATION -- ######################\n",
        "\n",
        "class CNNClassifier:\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.model = self._build_model(input_shape, num_classes)\n",
        "\n",
        "    def _build_model(self, input_shape, num_classes):\n",
        "        model = Sequential([\n",
        "            Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
        "            BatchNormalization(),\n",
        "            Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D((2, 2)),\n",
        "            Dropout(0.25),\n",
        "            Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D((2, 2)),\n",
        "            Dropout(0.25),\n",
        "            Flatten(),\n",
        "            Dense(512, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.5),\n",
        "            Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
        "        model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "    def train(self, x_train, y_train, x_val, y_val, epochs=50, batch_size=64):\n",
        "        datagen = ImageDataGenerator(\n",
        "            rotation_range=10,\n",
        "            zoom_range=0.1,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1\n",
        "        )\n",
        "        datagen.fit(x_train)\n",
        "        reduce_lr = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n",
        "        self.model.fit(\n",
        "            datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_val, y_val),\n",
        "            callbacks=[reduce_lr]\n",
        "        )\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        return self.model.predict(x_test)\n",
        "\n",
        "def startCNN(X, Y, input_shape, num_classes):\n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
        "    cnn = CNNClassifier(input_shape, num_classes)\n",
        "    cnn.train(X_train, Y_train, X_val, Y_val)\n",
        "    return cnn\n",
        "\n",
        "###################### -- XGBOOST Implementation -- ######################\n",
        "\n",
        "class XGBoostClassifier:\n",
        "    def __init__(self, max_depth=7, eta=0.1, num_class=24):\n",
        "        self.params = {\n",
        "            'objective': 'multi:softmax',\n",
        "            'num_class': num_class,\n",
        "            'booster': 'gbtree',\n",
        "            'eval_metric': 'merror',\n",
        "            'eta': eta,\n",
        "            'max_depth': max_depth,\n",
        "        }\n",
        "\n",
        "    def train(self, train_data, train_labels, validation_data, validation_labels):\n",
        "        dtrain = xgb.DMatrix(train_data, label=train_labels)\n",
        "        dval = xgb.DMatrix(validation_data, label=validation_labels)\n",
        "        watchlist = [(dtrain, 'train'), (dval, 'validation')]\n",
        "        self.model = xgb.train(self.params, dtrain, num_boost_round=200, evals=watchlist, early_stopping_rounds=20, verbose_eval=False)\n",
        "\n",
        "    def compute_predictions(self, data):\n",
        "        ddata = xgb.DMatrix(data)\n",
        "        return self.model.predict(ddata)\n",
        "\n",
        "    def compute_accuracy(self, preds, labels):\n",
        "        return np.mean(preds == labels)\n",
        "\n",
        "def startXGBOOST(train_data_normalized, train_labels, validation_data_normalized, validation_labels):\n",
        "    learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "    max_depths = [3, 4, 5, 6, 7, 8, 9, 10]\n",
        "    learning_rates = [0.1]\n",
        "    max_depths = [7]\n",
        "    results_XGB = []\n",
        "    best_acc_xgb = 0\n",
        "    best_lr = None\n",
        "    best_depth = None\n",
        "    num_class = len(np.unique(train_labels))\n",
        "    print(num_class)\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        for depth in max_depths:\n",
        "            xgb_model = XGBoostClassifier(max_depth=depth, eta=lr, num_class=num_class)\n",
        "            xgb_model.train(train_data_normalized, train_labels, validation_data_normalized, validation_labels)\n",
        "            val_preds = xgb_model.compute_predictions(validation_data_normalized)\n",
        "            acc = xgb_model.compute_accuracy(val_preds, validation_labels)\n",
        "            print(f\"LR: {lr} ; Depth: {depth} ; Accuracy: {acc * 100:.2f}%\")\n",
        "            if acc > best_acc_xgb:\n",
        "                best_acc_xgb = acc\n",
        "                best_lr = lr\n",
        "                best_depth = depth\n",
        "                print(\"New best\")\n",
        "            results_XGB.append((lr, depth, acc))\n",
        "\n",
        "    best_xgb_model = XGBoostClassifier(max_depth=best_depth, eta=best_lr, num_class=num_class)\n",
        "    best_xgb_model.train(train_data_normalized, train_labels, validation_data_normalized, validation_labels)\n",
        "\n",
        "    return best_xgb_model\n",
        "\n",
        "###################### -- DATA HANDLING AND TRAINING -- ######################\n",
        "\n",
        "# Load your dataset here\n",
        "mnist_sign_train = pd.read_csv('sign_mnist_train.csv')\n",
        "test_a, test_b = separate_test_sets('test.csv')\n",
        "\n",
        "mnist_sign_train['label'] = mnist_sign_train['label'].replace(24, 9) ## Because\n",
        "\n",
        "# Préparation des données de training pour XGBoost & CNN\n",
        "features_xgb = mnist_sign_train.drop('label', axis=1).values / 255.0\n",
        "labels_xgb = mnist_sign_train['label'].values\n",
        "features_cnn = features_xgb.reshape((-1, 28, 28, 1))\n",
        "labels_cnn = tf.keras.utils.to_categorical(labels_xgb, num_classes=25)\n",
        "\n",
        "# Préparation des données de test pour XGBoost & CNN\n",
        "normalized_test_a_cnn = test_a.values.reshape((-1, 28, 28, 1)) / 255.0\n",
        "normalized_test_b_cnn = test_b.values.reshape((-1, 28, 28, 1)) / 255.0\n",
        "normalized_test_a_xgb = test_a.values / 255.0\n",
        "normalized_test_b_xgb = test_b.values / 255.0\n",
        "\n",
        "#Split Training Validation pour XGB\n",
        "split_index = int(0.8 * len(features_xgb))\n",
        "x_train_xgb, x_val_xgb = features_xgb[:split_index], features_xgb[split_index:]\n",
        "y_train_xgb, y_val_xgb = labels_xgb[:split_index], labels_xgb[split_index:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR: 0.01 ; Depth: 3 ; Accuracy: 77.13%\n",
            "New best\n",
            "LR: 0.01 ; Depth: 4 ; Accuracy: 87.91%\n",
            "New best\n",
            "LR: 0.01 ; Depth: 5 ; Accuracy: 92.73%\n",
            "New best\n",
            "LR: 0.01 ; Depth: 6 ; Accuracy: 95.16%\n",
            "New best\n",
            "LR: 0.01 ; Depth: 7 ; Accuracy: 96.30%\n",
            "New best\n",
            "LR: 0.01 ; Depth: 8 ; Accuracy: 96.78%\n",
            "New best\n",
            "LR: 0.01 ; Depth: 9 ; Accuracy: 97.32%\n",
            "New best\n",
            "LR: 0.01 ; Depth: 10 ; Accuracy: 97.47%\n",
            "New best\n",
            "LR: 0.05 ; Depth: 3 ; Accuracy: 95.81%\n",
            "LR: 0.05 ; Depth: 4 ; Accuracy: 98.22%\n",
            "New best\n",
            "LR: 0.05 ; Depth: 5 ; Accuracy: 98.83%\n",
            "New best\n",
            "LR: 0.05 ; Depth: 6 ; Accuracy: 99.11%\n",
            "New best\n",
            "LR: 0.05 ; Depth: 7 ; Accuracy: 99.27%\n",
            "New best\n",
            "LR: 0.05 ; Depth: 8 ; Accuracy: 99.18%\n",
            "LR: 0.05 ; Depth: 9 ; Accuracy: 99.25%\n",
            "LR: 0.05 ; Depth: 10 ; Accuracy: 99.29%\n",
            "New best\n",
            "LR: 0.1 ; Depth: 3 ; Accuracy: 98.63%\n",
            "LR: 0.1 ; Depth: 4 ; Accuracy: 99.27%\n",
            "LR: 0.1 ; Depth: 5 ; Accuracy: 99.38%\n",
            "New best\n",
            "LR: 0.1 ; Depth: 6 ; Accuracy: 99.42%\n",
            "New best\n",
            "LR: 0.1 ; Depth: 7 ; Accuracy: 99.54%\n",
            "New best\n",
            "LR: 0.1 ; Depth: 8 ; Accuracy: 99.53%\n",
            "LR: 0.1 ; Depth: 9 ; Accuracy: 99.38%\n",
            "LR: 0.1 ; Depth: 10 ; Accuracy: 99.24%\n",
            "LR: 0.2 ; Depth: 3 ; Accuracy: 99.44%\n",
            "LR: 0.2 ; Depth: 4 ; Accuracy: 99.29%\n",
            "LR: 0.2 ; Depth: 5 ; Accuracy: 99.40%\n",
            "LR: 0.2 ; Depth: 6 ; Accuracy: 99.31%\n",
            "LR: 0.2 ; Depth: 7 ; Accuracy: 99.53%\n",
            "LR: 0.2 ; Depth: 8 ; Accuracy: 99.45%\n",
            "LR: 0.2 ; Depth: 9 ; Accuracy: 99.25%\n",
            "LR: 0.2 ; Depth: 10 ; Accuracy: 99.51%\n",
            "LR: 0.3 ; Depth: 3 ; Accuracy: 99.44%\n",
            "LR: 0.3 ; Depth: 4 ; Accuracy: 99.38%\n",
            "LR: 0.3 ; Depth: 5 ; Accuracy: 99.38%\n",
            "LR: 0.3 ; Depth: 6 ; Accuracy: 99.25%\n",
            "LR: 0.3 ; Depth: 7 ; Accuracy: 99.47%\n",
            "LR: 0.3 ; Depth: 8 ; Accuracy: 99.34%\n",
            "LR: 0.3 ; Depth: 9 ; Accuracy: 99.47%\n",
            "LR: 0.3 ; Depth: 10 ; Accuracy: 99.38%\n"
          ]
        }
      ],
      "source": [
        "# Train XGB Model\n",
        "best_xgb_model = startXGBOOST(x_train_xgb, y_train_xgb, x_val_xgb, y_val_xgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "387/387 [==============================] - 39s 97ms/step - loss: 1.0202 - accuracy: 0.6962 - val_loss: 3.4025 - val_accuracy: 0.2145 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "387/387 [==============================] - 36s 93ms/step - loss: 0.2316 - accuracy: 0.9254 - val_loss: 0.0188 - val_accuracy: 0.9975 - lr: 9.0000e-04\n",
            "Epoch 3/50\n",
            "387/387 [==============================] - 37s 94ms/step - loss: 0.1165 - accuracy: 0.9636 - val_loss: 0.2669 - val_accuracy: 0.8922 - lr: 8.1000e-04\n",
            "Epoch 4/50\n",
            "387/387 [==============================] - 36s 93ms/step - loss: 0.0828 - accuracy: 0.9739 - val_loss: 0.0025 - val_accuracy: 1.0000 - lr: 7.2900e-04\n",
            "Epoch 5/50\n",
            "387/387 [==============================] - 40s 103ms/step - loss: 0.0630 - accuracy: 0.9813 - val_loss: 0.0035 - val_accuracy: 1.0000 - lr: 6.5610e-04\n",
            "Epoch 6/50\n",
            "387/387 [==============================] - 38s 98ms/step - loss: 0.0501 - accuracy: 0.9853 - val_loss: 0.0013 - val_accuracy: 1.0000 - lr: 5.9049e-04\n",
            "Epoch 7/50\n",
            "387/387 [==============================] - 35s 89ms/step - loss: 0.0337 - accuracy: 0.9898 - val_loss: 4.0687e-04 - val_accuracy: 1.0000 - lr: 5.3144e-04\n",
            "Epoch 8/50\n",
            "387/387 [==============================] - 34s 88ms/step - loss: 0.0258 - accuracy: 0.9926 - val_loss: 1.2123e-04 - val_accuracy: 1.0000 - lr: 4.7830e-04\n",
            "Epoch 9/50\n",
            "387/387 [==============================] - 37s 96ms/step - loss: 0.0201 - accuracy: 0.9935 - val_loss: 1.2893e-04 - val_accuracy: 1.0000 - lr: 4.3047e-04\n",
            "Epoch 10/50\n",
            "387/387 [==============================] - 34s 89ms/step - loss: 0.0186 - accuracy: 0.9944 - val_loss: 0.0422 - val_accuracy: 0.9847 - lr: 3.8742e-04\n",
            "Epoch 11/50\n",
            "387/387 [==============================] - 34s 87ms/step - loss: 0.0228 - accuracy: 0.9936 - val_loss: 3.4788e-05 - val_accuracy: 1.0000 - lr: 3.4868e-04\n",
            "Epoch 12/50\n",
            "387/387 [==============================] - 33s 86ms/step - loss: 0.0145 - accuracy: 0.9960 - val_loss: 6.5752e-05 - val_accuracy: 1.0000 - lr: 3.1381e-04\n",
            "Epoch 13/50\n",
            "387/387 [==============================] - 34s 87ms/step - loss: 0.0109 - accuracy: 0.9970 - val_loss: 1.2230e-04 - val_accuracy: 1.0000 - lr: 2.8243e-04\n",
            "Epoch 14/50\n",
            "387/387 [==============================] - 36s 93ms/step - loss: 0.0100 - accuracy: 0.9971 - val_loss: 1.6613e-04 - val_accuracy: 1.0000 - lr: 2.5419e-04\n",
            "Epoch 15/50\n",
            "387/387 [==============================] - 35s 90ms/step - loss: 0.0134 - accuracy: 0.9962 - val_loss: 5.9261e-05 - val_accuracy: 1.0000 - lr: 2.2877e-04\n",
            "Epoch 16/50\n",
            "387/387 [==============================] - 35s 89ms/step - loss: 0.0110 - accuracy: 0.9970 - val_loss: 5.0020e-05 - val_accuracy: 1.0000 - lr: 2.0589e-04\n",
            "Epoch 17/50\n",
            "387/387 [==============================] - 35s 90ms/step - loss: 0.0099 - accuracy: 0.9973 - val_loss: 8.5617e-06 - val_accuracy: 1.0000 - lr: 1.8530e-04\n",
            "Epoch 18/50\n",
            "387/387 [==============================] - 35s 91ms/step - loss: 0.0081 - accuracy: 0.9978 - val_loss: 5.4791e-06 - val_accuracy: 1.0000 - lr: 1.6677e-04\n",
            "Epoch 19/50\n",
            "387/387 [==============================] - 35s 89ms/step - loss: 0.0078 - accuracy: 0.9981 - val_loss: 9.4565e-06 - val_accuracy: 1.0000 - lr: 1.5009e-04\n",
            "Epoch 20/50\n",
            "387/387 [==============================] - 34s 89ms/step - loss: 0.0062 - accuracy: 0.9984 - val_loss: 3.6518e-05 - val_accuracy: 1.0000 - lr: 1.3509e-04\n",
            "Epoch 21/50\n",
            "387/387 [==============================] - 33s 86ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 5.5505e-06 - val_accuracy: 1.0000 - lr: 1.2158e-04\n",
            "Epoch 22/50\n",
            "387/387 [==============================] - 34s 87ms/step - loss: 0.0055 - accuracy: 0.9980 - val_loss: 3.9045e-06 - val_accuracy: 1.0000 - lr: 1.0942e-04\n",
            "Epoch 23/50\n",
            "387/387 [==============================] - 33s 86ms/step - loss: 0.0065 - accuracy: 0.9980 - val_loss: 4.1911e-06 - val_accuracy: 1.0000 - lr: 9.8477e-05\n",
            "Epoch 24/50\n",
            "387/387 [==============================] - 33s 85ms/step - loss: 0.0066 - accuracy: 0.9980 - val_loss: 4.0141e-06 - val_accuracy: 1.0000 - lr: 8.8629e-05\n",
            "Epoch 25/50\n",
            "387/387 [==============================] - 37s 96ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 2.2736e-06 - val_accuracy: 1.0000 - lr: 7.9766e-05\n",
            "Epoch 26/50\n",
            "387/387 [==============================] - 36s 92ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 2.2375e-06 - val_accuracy: 1.0000 - lr: 7.1790e-05\n",
            "Epoch 27/50\n",
            "387/387 [==============================] - 35s 90ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 1.7661e-06 - val_accuracy: 1.0000 - lr: 6.4611e-05\n",
            "Epoch 28/50\n",
            "387/387 [==============================] - 34s 87ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 1.6396e-06 - val_accuracy: 1.0000 - lr: 5.8150e-05\n",
            "Epoch 29/50\n",
            "387/387 [==============================] - 34s 88ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 1.5723e-06 - val_accuracy: 1.0000 - lr: 5.2335e-05\n",
            "Epoch 30/50\n",
            "387/387 [==============================] - 34s 89ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 1.6602e-06 - val_accuracy: 1.0000 - lr: 4.7101e-05\n",
            "Epoch 31/50\n",
            "387/387 [==============================] - 35s 91ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 1.4018e-06 - val_accuracy: 1.0000 - lr: 4.2391e-05\n",
            "Epoch 32/50\n",
            "387/387 [==============================] - 36s 92ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 1.9054e-06 - val_accuracy: 1.0000 - lr: 3.8152e-05\n",
            "Epoch 33/50\n",
            "387/387 [==============================] - 36s 92ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 9.6443e-07 - val_accuracy: 1.0000 - lr: 3.4337e-05\n",
            "Epoch 34/50\n",
            "387/387 [==============================] - 35s 89ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 7.9843e-07 - val_accuracy: 1.0000 - lr: 3.0903e-05\n",
            "Epoch 35/50\n",
            "387/387 [==============================] - 34s 89ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 7.4642e-07 - val_accuracy: 1.0000 - lr: 2.7813e-05\n",
            "Epoch 36/50\n",
            "387/387 [==============================] - 36s 92ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 9.0960e-07 - val_accuracy: 1.0000 - lr: 2.5032e-05\n",
            "Epoch 37/50\n",
            "387/387 [==============================] - 36s 93ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 7.6300e-07 - val_accuracy: 1.0000 - lr: 2.2528e-05\n",
            "Epoch 38/50\n",
            "387/387 [==============================] - 35s 91ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 7.5501e-07 - val_accuracy: 1.0000 - lr: 2.0276e-05\n",
            "Epoch 39/50\n",
            "387/387 [==============================] - 35s 91ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 9.5601e-07 - val_accuracy: 1.0000 - lr: 1.8248e-05\n",
            "Epoch 40/50\n",
            "387/387 [==============================] - 36s 93ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 7.0366e-07 - val_accuracy: 1.0000 - lr: 1.6423e-05\n",
            "Epoch 41/50\n",
            "387/387 [==============================] - 35s 90ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 6.5226e-07 - val_accuracy: 1.0000 - lr: 1.4781e-05\n",
            "Epoch 42/50\n",
            "387/387 [==============================] - 35s 90ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 7.5706e-07 - val_accuracy: 1.0000 - lr: 1.3303e-05\n",
            "Epoch 43/50\n",
            "387/387 [==============================] - 35s 89ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 6.2704e-07 - val_accuracy: 1.0000 - lr: 1.1973e-05\n",
            "Epoch 44/50\n",
            "387/387 [==============================] - 34s 89ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 7.4607e-07 - val_accuracy: 1.0000 - lr: 1.0775e-05\n",
            "Epoch 45/50\n",
            "387/387 [==============================] - 35s 90ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 6.3833e-07 - val_accuracy: 1.0000 - lr: 9.6977e-06\n",
            "Epoch 46/50\n",
            "387/387 [==============================] - 35s 90ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 6.3720e-07 - val_accuracy: 1.0000 - lr: 8.7280e-06\n",
            "Epoch 47/50\n",
            "387/387 [==============================] - 34s 87ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 6.6997e-07 - val_accuracy: 1.0000 - lr: 7.8552e-06\n",
            "Epoch 48/50\n",
            "387/387 [==============================] - 35s 90ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 6.5139e-07 - val_accuracy: 1.0000 - lr: 7.0697e-06\n",
            "Epoch 49/50\n",
            "387/387 [==============================] - 34s 88ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 7.2029e-07 - val_accuracy: 1.0000 - lr: 6.3627e-06\n",
            "Epoch 50/50\n",
            "387/387 [==============================] - 33s 86ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 6.4024e-07 - val_accuracy: 1.0000 - lr: 5.7264e-06\n"
          ]
        }
      ],
      "source": [
        "# Train CNN Model\n",
        "cnn_model = startCNN(features_cnn, labels_cnn, input_shape=(28, 28, 1), num_classes=25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost Predictions\n",
        "xgb_preds_a = best_xgb_model.compute_predictions(normalized_test_a_xgb)\n",
        "xgb_preds_b = best_xgb_model.compute_predictions(normalized_test_b_xgb)\n",
        "xgb_merged_predictions = merge_predictions(xgb_preds_a, xgb_preds_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "94/94 [==============================] - 1s 8ms/step\n",
            "94/94 [==============================] - 1s 8ms/step\n"
          ]
        }
      ],
      "source": [
        "# CNN Predictions\n",
        "cnn_preds_a = cnn_model.predict(normalized_test_a_cnn)\n",
        "cnn_preds_b = cnn_model.predict(normalized_test_b_cnn)\n",
        "cnn_merged_predictions = merge_predictions(np.argmax(cnn_preds_a, axis=1), np.argmax(cnn_preds_b, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save XGB\n",
        "save_predictions_to_csv(\"xgb_predictions.csv\", xgb_merged_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save CNN\n",
        "save_predictions_to_csv(\"cnn_predictions.csv\", cnn_merged_predictions)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
